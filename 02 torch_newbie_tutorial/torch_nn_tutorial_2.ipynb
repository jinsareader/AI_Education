{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6a6800-5967-40aa-8bae-a7361bc26b81",
   "metadata": {},
   "source": [
    "### torch.nn 의 여러 함수들의 기능에 대해\n",
    "\n",
    "torch.nn.Linear(in_features, out_features, bias) (선형함수) y = xW + b\n",
    "\n",
    "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias) (합성곱 함수) y = x*W + b (x와 W는 곱하는거 아님)\n",
    "kernel_size : W의 크기\n",
    "\n",
    "torch.nn.RNN(input_size, hidden_size, num_layers, bias, batch_first) (재귀) h = tanh(xW + hW + b)\n",
    "batch_first == True : (b, s, f)  \n",
    "batch_first == False : (s, b, f)\n",
    "b : 문장, s : 단어, f : 단어의 벡터값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33712ae-b638-42f2-8453-0f7a1a8c5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20c8953a-57e4-4282-b844-b6c41cff1df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.6147]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.9318]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9318]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.9318]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)  #랜덤 시드 고정\n",
    "\n",
    "linear_layer = nn.Linear(10, 1)\n",
    "\n",
    "input_tensor = torch.tensor([[1,2,3,4,5,6,7,8,9,10]], dtype = torch.float)\n",
    "\n",
    "linear_output = linear_layer(input_tensor)\n",
    "print(linear_output)\n",
    "\n",
    "#시그모이드 함수 적용 0 ~ 1 이이\n",
    "#functional 모듈 : 함수 계산을 객체 선언없이 바로 하고싶을 때\n",
    "\n",
    "sigmoid_output1 = torch.nn.functional.sigmoid(linear_output)\n",
    "print(sigmoid_output1)\n",
    "\n",
    "sigmoid_function = torch.nn.Sigmoid()\n",
    "sigmoid_output2 = sigmoid_function(linear_output)\n",
    "print(sigmoid_output2)\n",
    "\n",
    "\n",
    "# sigmoid 식 = 1 / (1+exp(-x))\n",
    "sigmoid_output3 = 1 / (1+torch.exp(-1 * linear_output))\n",
    "print(sigmoid_output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a27dab1-5261-4e62-9410-7dcede443696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0832]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0830]], grad_fn=<TanhBackward0>)\n",
      "tensor([[-0.0830]], grad_fn=<TanhBackward0>)\n",
      "tensor([[-0.0830]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(9) #시드 고정\n",
    "\n",
    "linear_layer = nn.Linear(10,1)\n",
    "input_tensor = torch.randn(1,10) #1행 10열의 랜덤 2차원 tensor 생성 (0-1)\n",
    "\n",
    "linear_output = linear_layer(input_tensor)\n",
    "print(linear_output)\n",
    "\n",
    "#tanh -1 ~ 1 사이의 값\n",
    "tanh_output = nn.functional.tanh(linear_output)\n",
    "print(tanh_output)\n",
    "\n",
    "tanh_function = nn.Tanh()\n",
    "tanh_output = tanh_function(linear_output)\n",
    "print(tanh_output)\n",
    "\n",
    "#tanh = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "tanh_output = (torch.exp(linear_output) - torch.exp(-linear_output)) / (torch.exp(linear_output) + torch.exp(-linear_output))\n",
    "print(tanh_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7d2d2b9-b746-4b79-8c7f-56cc8de70905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0657,  0.2302,  0.0756, -0.4304, -0.0321]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0657, 0.2302, 0.0756, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "tensor([[0.0657, 0.2302, 0.0756, 0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12)\n",
    "\n",
    "linear_layer = nn.Linear(10,5)\n",
    "input_tensor = torch.rand(1,10)\n",
    "\n",
    "linear_output = linear_layer(input_tensor)\n",
    "print(linear_output)\n",
    "\n",
    "#ReLU : 양수는 그대로, 음수는 0\n",
    "ReLU_output = nn.functional.relu(linear_output)\n",
    "print(ReLU_output)\n",
    "\n",
    "ReLU = nn.ReLU()\n",
    "ReLU_output = ReLU(linear_output)\n",
    "print(ReLU_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6c87164-7fce-457c-ac77-fe25dfd51e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4261, -0.3705,  0.8186,  0.0261,  0.8250]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.4261, -0.0037,  0.8186,  0.0261,  0.8250]],\n",
      "       grad_fn=<LeakyReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12)\n",
    "\n",
    "linear_layer = nn.Linear(10,5)\n",
    "input_tensor = torch.randn(1,10)\n",
    "\n",
    "linear_output = linear_layer(input_tensor)\n",
    "print(linear_output)\n",
    "\n",
    "#LeakyReLU : 양수는 그대로, 음수는 negative_slope 곱한 값\n",
    "leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "print(leaky_relu(linear_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38a077da-129f-41aa-b402-8ec8957839c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4240, -0.2668,  0.6506,  0.7172,  0.4744, -0.8246, -0.7034,  0.3599,\n",
      "         -0.0691, -0.0794]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.1264, 0.0633, 0.1585, 0.1695, 0.1329, 0.0363, 0.0409, 0.1185, 0.0772,\n",
      "         0.0764]], grad_fn=<SoftmaxBackward0>)\n",
      "softmax 함수의 전체 합 :  tensor(1.0000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(999)\n",
    "\n",
    "linear_layer = nn.Linear(10,10)\n",
    "input_tensor = torch.randn(1,10)\n",
    "\n",
    "linear_output = linear_layer(input_tensor)\n",
    "print(linear_output)\n",
    "\n",
    "# 소프트 맥수 함수 #0~1의 값, 모든 값 합치면 1, 해당 활성화함수는 출력층에서 확률로 변환 할 때 쓰임\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "softmax_output = softmax(linear_output)\n",
    "print(softmax_output)\n",
    "print(\"softmax 함수의 전체 합 : \",softmax_output.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3913b2f8-b924-47a0-b7de-c59dfd05858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "linear_layer = nn.Linear(10,1) #1행 1열\n",
    "input_tensor = torch.randn(1,10) #1행 10열\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "\n",
    "print(softmax(linear_layer(input_tensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e5df4c7-d6d1-4f3b-905c-dce92840cd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data :  tensor([[-0.6606,  0.7300,  0.0792, -0.1249,  0.0435, -0.5388, -0.3659, -0.0940,\n",
      "         -0.5789,  1.0222]])\n",
      "output data :  tensor([[-0.8258,  0.9125,  0.0991, -0.1562,  0.0544, -0.6735, -0.4573, -0.1175,\n",
      "         -0.7236,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "#드랍 아웃 : 함수에서 W를 몇개 비활성화 할지 '비율로' 정하는 수치\n",
    "\n",
    "dropout = nn.Dropout(p = 0.2)\n",
    "#20%의 확률로 값을 비활성화 (0으로 만듬), 살아남은 값들은 (1 / (1-p))를 곱해서 값을 보정함\n",
    "input_data = torch.randn(1,10)\n",
    "\n",
    "output = dropout(input_data)\n",
    "\n",
    "print(\"input data : \", input_data)\n",
    "print(\"output data : \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "728fa924-74b3-450a-9147-c4d59f8b729b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.2, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (6): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Sequential 신경망 정의\n",
    "\n",
    "#f(784 > 128) -> ReLU() -> Dropout(0.2) -> g(128 > 64) -> ReLU() -> h(64 > 10) -> Softmax() 로 진행되는 신경망 함수 만기기\n",
    "\n",
    "F = nn.Sequential(\n",
    "    nn.Linear(784,128), #f\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p = 0.2),\n",
    "    nn.Linear(128,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,10),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19db996-ffa2-477c-aa79-61416cc2cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## custom 클래스로 위의 Sequential 신경망과 똑같이 작동하는것 만들기\n",
    "\n",
    "class custom_NN(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super().__init__()\n",
    "        self.f = nn.Linear(784, 128)\n",
    "        self.g = nn.Linear(128,64)\n",
    "        self.h = nn.Linear(64,10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p = 0.2)\n",
    "        self.softmax = nn.Softmax()\n",
    "    def forward(self, x) :\n",
    "        x = self.f(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.g(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.h(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f07c0ee1-0259-404c-8c4a-fe3ed0c4bdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1000, 0.7747]])\n",
      "tensor([[1.1000, 0.7747]])\n"
     ]
    }
   ],
   "source": [
    "##nn.functional 여기서도 객체 선언없이 계산이 가능하다\n",
    "# 일반 활성화 함수 뿐만 아니라, 노드에 쓰이는 함수들(선형함수, cnn, rnn)같은 것들도 계산이 가능\n",
    "\n",
    "### functional로 선형함수\n",
    "\n",
    "input_data = torch.rand(1,3) #1행 3열의 2차원 tensor\n",
    "\n",
    "weight = torch.rand(2,3)\n",
    "bias = torch.rand(2)\n",
    "\n",
    "output = nn.functional.linear(input_data, weight, bias)\n",
    "print(output)\n",
    "\n",
    "output = torch.matmul(input_data, weight.T) + bias # y = x*W.T + b\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5da1dee9-d5e1-4f31-8f03-0a30e37729e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4512, -0.5088, -0.1428],\n",
      "        [-0.3730,  0.4167, -0.2049]], requires_grad=True)\n",
      "torch.Size([2, 3])\n",
      "Parameter containing:\n",
      "tensor([-0.1840, -0.1130], requires_grad=True)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "linear_layer = nn.Linear(3,2)\n",
    "\n",
    "for i in linear_layer.parameters() :\n",
    "    print(i)\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5cad9019-597c-451e-a5b8-e8c92e994846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "### functional로 CNN\n",
    "input_data = torch.rand(1,1,5,5) #(그림 1장, 색깔채널은 1개, 가로 픽셀 5, 세로 픽셀 5)\n",
    "\n",
    "weight = torch.rand(16,1,3,3) #(출력 채녈 16개, 갯수 맞춰주는 것 , kernal 가로 픽셀 3, kernel 세로 픽셀 3)\n",
    "\n",
    "bias = torch.rand(16)\n",
    "\n",
    "output = nn.functional.conv2d(input_data, weight, bias, stride = 1, padding = 0)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "883e19cd-3a59-4e2f-ad7b-868fd0bdfdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0000,  0.9127,  0.0000],\n",
      "        [-0.0000,  0.0000,  1.9669],\n",
      "        [-0.3214,  0.0000, -1.2753]])\n",
      "tensor([[-0.8379,  0.4564,  0.3481],\n",
      "        [-0.1371,  0.2202,  0.9835],\n",
      "        [-0.1607,  0.9681, -0.6377]])\n"
     ]
    }
   ],
   "source": [
    "### functional로 dropout 구현\n",
    "\n",
    "torch.manual_seed(999)\n",
    "\n",
    "input_tensor = torch.randn(3,3)\n",
    "\n",
    "##dropout은 학습할때는 켜 두고 실제 테스트 할때는 꺼 둡니다.\n",
    "\n",
    "output1 = nn.functional.dropout(input_tensor, p=0.5, training = True) #training : dropout을 적용할 지 안할 지 판단\n",
    "print(output1)\n",
    "\n",
    "output2 = nn.functional.dropout(input_tensor, p=0.5, training = False)\n",
    "print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "856fad2b-7299-45ab-aee7-ac3e0097636b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 1.4660, 0.0000, 0.0000, 0.1387],\n",
      "        [0.2974, 0.0000, 0.9461, 0.0000, 0.0000]])\n",
      "tensor([[0.9817, 0.8796, 0.9921, 0.4611, 0.0832],\n",
      "        [0.1784, 0.3674, 0.5676, 0.3376, 0.2119]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12345)\n",
    "\n",
    "F = nn.Sequential(\n",
    "    nn.Dropout(p = 0.4)\n",
    ")\n",
    "\n",
    "test_tensor = torch.rand(2,5)\n",
    "\n",
    "F.train() #F 의 Dropout을 활성화, 학습 과정중에 사용\n",
    "print(F(test_tensor))\n",
    "F.eval() #F 의 Dropout을 비활성화, 검증 과정중에 사용\n",
    "print(F(test_tensor))\n",
    "\n",
    "#신경망 내부에 dropout이 존재하면, dropout을 켜고 끄는 작업이 필요!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a58f009d-8335-4316-ab69-8e55140adf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100 || loss 5.5979323387146\n",
      "epoch 200 || loss 3.913304090499878\n",
      "epoch 300 || loss 2.7636098861694336\n",
      "epoch 400 || loss 1.9711214303970337\n",
      "epoch 500 || loss 1.419503927230835\n",
      "epoch 600 || loss 1.031815767288208\n",
      "epoch 700 || loss 0.7567359209060669\n",
      "epoch 800 || loss 0.5597349405288696\n",
      "epoch 900 || loss 0.4173768162727356\n",
      "epoch 1000 || loss 0.31360992789268494\n"
     ]
    }
   ],
   "source": [
    "## nn.functional를 이용한 신경망 커스텀 클래스 생성\n",
    "torch.manual_seed(98765)\n",
    "\n",
    "class custom_NN(nn.Module) :\n",
    "    def __init__(self, input_size, output_size) :\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(output_size, input_size))\n",
    "        self.bias = nn.Parameter(torch.randn(output_size))\n",
    "\n",
    "    def forward(self, x) :\n",
    "        x = nn.functional.linear(x, self.weight, self.bias) # = x*weight.T + bias\n",
    "        return x\n",
    "\n",
    "F = custom_NN(100,10)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(F.parameters() ,lr = 0.001)\n",
    "\n",
    "tensor_x = torch.rand(10,100)\n",
    "tensor_t = torch.rand(10,10)\n",
    "epoch = 1000\n",
    "\n",
    "for e in range(epoch) :\n",
    "    loss_sum = 0\n",
    "    for b in range(len(tensor_x)) :\n",
    "        y = F(tensor_x[b])\n",
    "\n",
    "        loss = loss_function(y, tensor_t[b])\n",
    "        loss_sum += loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (e+1) % 100 == 0 :\n",
    "        print(f\"epoch {e+1} || loss {loss_sum / len(tensor_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba204a93-83a6-42e5-92d0-06a1bb7d9021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
