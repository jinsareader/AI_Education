{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80jB9KK6qJsA",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1747315126121,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "80jB9KK6qJsA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "R0evR9dHqLZF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1414,
     "status": "ok",
     "timestamp": 1747315127540,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "R0evR9dHqLZF",
    "outputId": "ac4f9034-1b07-46b1-d983-9922e1694a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "NG3Ru47bqQUJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747315127541,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "NG3Ru47bqQUJ",
    "outputId": "e772b717-3f29-4c60-b4d2-ad6d426583c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/PyAI/16 Seq2Seq\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/PyAI/16 Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "387e3741-670c-4724-a40d-09d0e708998a",
   "metadata": {
    "executionInfo": {
     "elapsed": 4090,
     "status": "ok",
     "timestamp": 1747315131629,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "387e3741-670c-4724-a40d-09d0e708998a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"\"), \"..\"))\n",
    "\n",
    "import custom\n",
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f03e46a-dd49-4cff-acbe-d8415fdfa914",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1747315131653,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "6f03e46a-dd49-4cff-acbe-d8415fdfa914",
    "outputId": "8206259c-3b0c-44b5-86ba-03bb361831f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
      "{' ': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '0': 10, '+': 11}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# [\" \",1,2,3,4,5,6,7,8,9,0,+]\n",
    "\n",
    "embedding_tensor = torch.cat([torch.zeros(1,11),torch.eye(11)])\n",
    "print(embedding_tensor)\n",
    "\n",
    "\n",
    "dic = {}\n",
    "\n",
    "dic[\" \"] = 0\n",
    "for i in range(1,10) :\n",
    "    dic[str(i)] = i\n",
    "dic[\"0\"] = 10\n",
    "dic[\"+\"] = 11\n",
    "\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a8850c-3388-47da-bc53-24a7813cfdec",
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1747315131694,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "66a8850c-3388-47da-bc53-24a7813cfdec"
   },
   "outputs": [],
   "source": [
    "with open(\"addition.txt\", mode = \"r\") as f:\n",
    "    ori_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92a0923c-6a67-4c52-9e5c-5a10e47b0fb0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1747315131847,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "92a0923c-6a67-4c52-9e5c-5a10e47b0fb0",
    "outputId": "65bc4c0b-ed1f-4898-f210-24110b7516ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "7\n",
      "['1', '6', '+', '7', '5', ' ', ' ']\n",
      "\n",
      "50000\n",
      "4\n",
      "['9', '1', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "data = ori_data.split(\"\\n\")\n",
    "\n",
    "x = []\n",
    "t = []\n",
    "for nums in data :\n",
    "    if nums.find(\"_\") < 0 :\n",
    "        continue\n",
    "    nums = nums.split(\"_\")\n",
    "    x.append(nums[0])\n",
    "    t.append(nums[1])\n",
    "\n",
    "print(len(x))\n",
    "print(len(x[0]))\n",
    "print(list(x[0]))\n",
    "print()\n",
    "print(len(t))\n",
    "print(len(t[0]))\n",
    "print(list(t[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beae79af-c12b-44cf-b45b-0293413770f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 435,
     "status": "ok",
     "timestamp": 1747315132320,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "beae79af-c12b-44cf-b45b-0293413770f2",
    "outputId": "f5dfe35f-a36b-442b-da90-a106863df40e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 7)\n"
     ]
    }
   ],
   "source": [
    "vector_x = []\n",
    "for i in range(len(x)) :\n",
    "    temp = list(x[i])\n",
    "    vector = custom.word_vectorize(temp, dic, 7, pad_word = \" \", unk_word = \" \")\n",
    "    vector_x.append(vector)\n",
    "\n",
    "vector_x = numpy.array(vector_x)\n",
    "print(vector_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b682b7d-fee7-47a5-a08d-4e9937ff1c1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 666,
     "status": "ok",
     "timestamp": 1747315132987,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "6b682b7d-fee7-47a5-a08d-4e9937ff1c1b",
    "outputId": "5faaa1c4-bcb7-412e-b10d-0e2295832212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 4)\n"
     ]
    }
   ],
   "source": [
    "vector_t = []\n",
    "for i in range(len(t)) :\n",
    "    temp = list(t[i])\n",
    "    vector = custom.word_vectorize(temp, dic, 4)\n",
    "    vector_t.append(vector)\n",
    "\n",
    "vector_t = numpy.array(vector_t)\n",
    "print(vector_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e00ea13f-249c-4483-b544-74fa0d1b62da",
   "metadata": {
    "executionInfo": {
     "elapsed": 770,
     "status": "ok",
     "timestamp": 1747315133759,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "e00ea13f-249c-4483-b544-74fa0d1b62da"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tensor_x = torch.tensor(vector_x, dtype = torch.long, device = device)\n",
    "tensor_t = torch.tensor(vector_t, dtype = torch.long, device = device)\n",
    "\n",
    "train_zip_list = list(zip(tensor_x[:40000], tensor_t[:40000]))\n",
    "test_zip_list = list(zip(tensor_x[40000:], tensor_t[40000:]))\n",
    "\n",
    "train_dataloader = DataLoader(train_zip_list, batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(test_zip_list, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c31639-4801-40e7-b44f-be23a0fd137b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 386673,
     "status": "ok",
     "timestamp": 1747315520441,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "16c31639-4801-40e7-b44f-be23a0fd137b",
    "outputId": "dfe86349-1118-4e31-bf8d-f3d4e4a7e4a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | loss 1.5997299319505691 | acc 0.441275 | cnt 0\n",
      "epoch 2 | loss 1.2016075259447099 | acc 0.520475 | cnt 0\n",
      "epoch 3 | loss 1.0648012913763523 | acc 0.5576 | cnt 0\n",
      "epoch 4 | loss 0.9989162389934063 | acc 0.5622 | cnt 0\n",
      "epoch 5 | loss 0.9474339802563191 | acc 0.5908 | cnt 0\n",
      "epoch 6 | loss 0.9154964350163937 | acc 0.60115 | cnt 0\n",
      "epoch 7 | loss 0.8827401274442672 | acc 0.603775 | cnt 0\n",
      "epoch 8 | loss 0.8632310445606709 | acc 0.6053 | cnt 0\n",
      "epoch 9 | loss 0.8411504009366035 | acc 0.62265 | cnt 0\n",
      "epoch 10 | loss 0.8173815773427486 | acc 0.622725 | cnt 0\n",
      "epoch 11 | loss 0.7925164860486984 | acc 0.6297 | cnt 0\n",
      "epoch 12 | loss 0.7558232551813125 | acc 0.6429 | cnt 0\n",
      "epoch 13 | loss 0.7250418528914452 | acc 0.652775 | cnt 0\n",
      "epoch 14 | loss 0.6987292501330375 | acc 0.6668 | cnt 0\n",
      "epoch 15 | loss 0.6633732752501964 | acc 0.6735 | cnt 0\n",
      "epoch 16 | loss 0.6292708474397659 | acc 0.68695 | cnt 0\n",
      "epoch 17 | loss 0.5983217945694923 | acc 0.693925 | cnt 0\n",
      "epoch 18 | loss 0.5752841271460056 | acc 0.70065 | cnt 0\n",
      "epoch 19 | loss 0.5556650832295418 | acc 0.714 | cnt 0\n",
      "epoch 20 | loss 0.5298214893788099 | acc 0.72265 | cnt 0\n",
      "epoch 21 | loss 0.5186319021880627 | acc 0.730925 | cnt 0\n",
      "epoch 22 | loss 0.5051300504058599 | acc 0.745125 | cnt 0\n",
      "epoch 23 | loss 0.48356385610997676 | acc 0.74475 | cnt 1\n",
      "epoch 24 | loss 0.47212126322090625 | acc 0.740975 | cnt 2\n",
      "epoch 25 | loss 0.4647019673138857 | acc 0.776875 | cnt 0\n",
      "epoch 26 | loss 0.43917525708675387 | acc 0.771275 | cnt 1\n",
      "epoch 27 | loss 0.4277157712727785 | acc 0.794975 | cnt 0\n",
      "epoch 28 | loss 0.40588524378836155 | acc 0.797375 | cnt 0\n",
      "epoch 29 | loss 0.39496401257812974 | acc 0.7763 | cnt 1\n",
      "epoch 30 | loss 0.3801209981739521 | acc 0.806625 | cnt 0\n",
      "epoch 31 | loss 0.3644862561672926 | acc 0.80555 | cnt 1\n",
      "epoch 32 | loss 0.34226024851202963 | acc 0.82835 | cnt 0\n",
      "epoch 33 | loss 0.33586363181471823 | acc 0.83995 | cnt 0\n",
      "epoch 34 | loss 0.3119182346761227 | acc 0.847125 | cnt 0\n",
      "epoch 35 | loss 0.3019762271270156 | acc 0.8518 | cnt 0\n",
      "epoch 36 | loss 0.2856738852337003 | acc 0.86405 | cnt 0\n",
      "epoch 37 | loss 0.2788709418848157 | acc 0.86555 | cnt 0\n",
      "epoch 38 | loss 0.26950075291097164 | acc 0.867475 | cnt 0\n",
      "epoch 39 | loss 0.2575016748160124 | acc 0.8606 | cnt 1\n",
      "epoch 40 | loss 0.24820212874561548 | acc 0.889275 | cnt 0\n",
      "epoch 41 | loss 0.24292282450944186 | acc 0.866525 | cnt 1\n",
      "epoch 42 | loss 0.2449233666062355 | acc 0.884475 | cnt 2\n",
      "epoch 43 | loss 0.22778559751808644 | acc 0.89325 | cnt 0\n",
      "epoch 44 | loss 0.23341139767318964 | acc 0.89765 | cnt 0\n",
      "epoch 45 | loss 0.2205350563302636 | acc 0.891125 | cnt 1\n",
      "epoch 46 | loss 0.2193671278283 | acc 0.895125 | cnt 2\n",
      "epoch 47 | loss 0.2070778190717101 | acc 0.88965 | cnt 3\n",
      "epoch 48 | loss 0.21875064890831708 | acc 0.8897 | cnt 4\n",
      "epoch 49 | loss 0.2032464813068509 | acc 0.899925 | cnt 0\n",
      "epoch 50 | loss 0.20037163130939006 | acc 0.8848 | cnt 1\n",
      "epoch 51 | loss 0.19828105136752128 | acc 0.898325 | cnt 2\n",
      "epoch 52 | loss 0.200667857080698 | acc 0.897925 | cnt 3\n",
      "epoch 53 | loss 0.192684825938195 | acc 0.89725 | cnt 4\n",
      "epoch 54 | loss 0.18935287326574327 | acc 0.9079 | cnt 0\n",
      "epoch 55 | loss 0.19026108225807548 | acc 0.910425 | cnt 0\n",
      "epoch 56 | loss 0.18195421062409878 | acc 0.91305 | cnt 0\n",
      "epoch 57 | loss 0.18834152510389685 | acc 0.9095 | cnt 1\n",
      "epoch 58 | loss 0.17957646312192083 | acc 0.913275 | cnt 0\n",
      "epoch 59 | loss 0.1767505950294435 | acc 0.922175 | cnt 0\n",
      "epoch 60 | loss 0.17661163449287415 | acc 0.9128 | cnt 1\n",
      "epoch 61 | loss 0.1768357422016561 | acc 0.917775 | cnt 2\n",
      "epoch 62 | loss 0.16784266140311957 | acc 0.91505 | cnt 3\n",
      "epoch 63 | loss 0.17054995927959682 | acc 0.887975 | cnt 4\n",
      "epoch 64 | loss 0.16952701361849903 | acc 0.9162 | cnt 5\n",
      "train stopped\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_tensor):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=True, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embedding_tensor.shape[1], embedding_tensor.shape[1], batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.flip(x, [-1])\n",
    "        x = self.embedding(x)\n",
    "        output, hc = self.rnn(x)\n",
    "        return output, hc\n",
    "\n",
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, embedding_tensor):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_tensor, freeze=True, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(embedding_tensor.shape[1] * 3, embedding_tensor.shape[1], batch_first=True, bidirectional=True)\n",
    "        self.f = nn.Linear(embedding_tensor.shape[1] * 4, embedding_tensor.shape[0])\n",
    "        self.encoder_h_context = None\n",
    "\n",
    "    def forward(self, encoder_output, encoder_hc, t = None) :\n",
    "        encoder_h_forward = encoder_hc[0][0:1,:,:]\n",
    "        encoder_h_backward = encoder_hc[0][1:2,:,:]\n",
    "        self.encoder_h_context = torch.concat([encoder_h_forward,encoder_h_backward], dim = -1).transpose(0,1)\n",
    "        batch_size = encoder_output.shape[0]\n",
    "        decoder_input = torch.zeros(batch_size, 1).type(torch.long).to(encoder_output.device)\n",
    "        decoder_hc = encoder_hc\n",
    "        decoder_output_list = []\n",
    "\n",
    "        for i in range(4) :\n",
    "            decoder_output, decoder_hc = self.forward_sub(decoder_input, decoder_hc)\n",
    "            decoder_output_list.append(decoder_output)\n",
    "\n",
    "            if t is None :\n",
    "                decoder_input = decoder_output.argmax(dim = -1).detach()\n",
    "            else :\n",
    "                decoder_input = t[:, i].unsqueeze(-1)\n",
    "\n",
    "        decoder_output_list = torch.cat(decoder_output_list, dim=1)\n",
    "        return decoder_output_list, decoder_hc, None\n",
    "\n",
    "    def forward_sub(self, x, h) :\n",
    "        x = self.embedding(x)\n",
    "        x = torch.concat([self.encoder_h_context, x], dim = -1)\n",
    "        output, hc = self.rnn(x, h)\n",
    "        output = torch.concat([self.encoder_h_context, output], dim = -1)\n",
    "        output = self.f(output)\n",
    "        return output, hc\n",
    "\n",
    "encoder = Encoder(embedding_tensor)\n",
    "decoder = Decoder(embedding_tensor)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr = 0.02)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr = 0.02)\n",
    "epoch = 100\n",
    "prev_acc = 0\n",
    "acc_cnt = 0\n",
    "\n",
    "for e in range(epoch) :\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    loss_sum = 0\n",
    "    for x, t in train_dataloader :\n",
    "        encoder_output, encoder_h = encoder(x)\n",
    "        y, _ , _ = decoder(encoder_output, encoder_h, t)\n",
    "\n",
    "        loss = loss_function(y.reshape(-1, y.shape[-1]), t.reshape(-1))\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "    loss_sum /= len(train_dataloader)\n",
    "    # print(f\"epoch {e+1} | loss {loss_sum}\")\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, t in test_dataloader :\n",
    "        encoder_output, encoder_h = encoder(x)\n",
    "        y, _ , _ = decoder(encoder_output, encoder_h, None)\n",
    "        for i in range(len(x)) :\n",
    "            for j in range(4) :\n",
    "                if (y[i][j].argmax() == t[i][j]) :\n",
    "                    correct += 1\n",
    "        total += len(x) * 4\n",
    "    acc = correct / total\n",
    "\n",
    "    if acc <= prev_acc :\n",
    "        acc_cnt += 1\n",
    "    else :\n",
    "        acc_cnt = 0\n",
    "        prev_acc = acc\n",
    "    print(f\"epoch {e+1} | loss {loss_sum} | acc {acc} | cnt {acc_cnt}\")\n",
    "    if acc_cnt >= 5 :\n",
    "        print(\"train stopped\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52a83a12-bcef-4eaa-b51e-3b51a66ab61e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1747315520463,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "52a83a12-bcef-4eaa-b51e-3b51a66ab61e",
    "outputId": "fc65aded-0c56-4fff-a110-d3af5233a0bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77  "
     ]
    }
   ],
   "source": [
    "test = \"31+47\"\n",
    "test_vector = [custom.word_vectorize(list(test), dic, 7, padding_front=False, pad_word = \" \", unk_word = \" \")]\n",
    "test_tensor = torch.tensor(test_vector, dtype = torch.long, device = device)\n",
    "\n",
    "x, h = encoder(test_tensor)\n",
    "ys, _, _ = decoder(x, h)\n",
    "for y in ys[0] :\n",
    "    print(list(dic.keys())[y.argmax().item()], end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "515bccfe-b3d9-44f0-ad3b-403705ecf011",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1747315520476,
     "user": {
      "displayName": "Yong-Jun Jang",
      "userId": "12216012798125380557"
     },
     "user_tz": -540
    },
    "id": "515bccfe-b3d9-44f0-ad3b-403705ecf011"
   },
   "outputs": [],
   "source": [
    "torch.save(encoder.to(\"cpu\"), \"num_encoder.pt\")\n",
    "torch.save(decoder.to(\"cpu\"), \"num_decoder.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
