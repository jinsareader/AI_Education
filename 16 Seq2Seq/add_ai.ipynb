{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5319f009-ecce-41f7-957d-c120299e80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 덧셈만 하는 deeplearing ai 만들기\n",
    "# 숫자값을 int가 아니라 str 문자 하나로 생각해서 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e04307-1813-4cea-a87a-df93604caf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#필요한 module import\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"\"), \"..\"))\n",
    "\n",
    "import custom\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f83ec98-fbb2-4551-b35e-3b27cc02af92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기 전에 원핫 인코딩 리스트, 숫자-라벨링 사전 두개 만들어서 원핫 인코딩 리스트는 신경망 안에 집어넣기\n",
    "#[' ',1,2,3,4,5,6,7,8,9,0,+]\n",
    "\n",
    "#원한 인코딩 리스트\n",
    "embedding_tensor = torch.cat([torch.zeros(1,11), torch.eye(11)])\n",
    "#torch.zeros : 파라미터 크기의 0텐서 생성\n",
    "#torch.eye : 정사각 단위행렬 (대각선이 1이고 나머진 0인 행렬) 생성\n",
    "print(embedding_tensor)\n",
    "#0번째 : 공백(padding), 1~10번째 : 1~0, 11번째 : +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272801aa-1a6d-44d7-94e4-e861293f421a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '0': 10, '+': 11}\n"
     ]
    }
   ],
   "source": [
    "#숫자 문자 - 라벨링 사전\n",
    "\n",
    "num_dict = {}\n",
    "\n",
    "num_dict[\" \"] = 0\n",
    "for i in range(1,10) :\n",
    "    num_dict[str(i)] = i\n",
    "num_dict[\"0\"] = 10\n",
    "num_dict[\"+\"] = 11\n",
    "\n",
    "print(num_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0be3001-ded7-41d4-b911-7ded0cda4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽어오기\n",
    "\n",
    "with open(\"addition.txt\", mode = \"r\") as f:\n",
    "    ori_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd19ef0-fc32-4a0b-9bfe-0e61af7018ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x의 갯수 :  50000\n",
      "x문자의 갯수 :  7\n",
      "x를 문자로 나눈 것 :  ['1', '8', '+', '8', ' ', ' ', ' ']\n",
      "t의 갯수 :  50000\n",
      "t문자의 갯수 :  4\n",
      "t를 문자로 나눈 것 :  ['2', '6', ' ', ' ']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 x, t로 나누기\n",
    "data = ori_data.split(\"\\n\")\n",
    "\n",
    "x = []\n",
    "t = []\n",
    "for nums in data :\n",
    "    if nums.find(\"_\") < 0 :\n",
    "        continue\n",
    "    nums = nums.split(\"_\") # _를 기준으로 나누고\n",
    "    x.append(nums[0]) #앞의 입력값\n",
    "    t.append(nums[1]) #뒤의 출력값\n",
    "\n",
    "print(\"x의 갯수 : \", len(x))\n",
    "print(\"x문자의 갯수 : \", len(x[10]))\n",
    "print(\"x를 문자로 나눈 것 : \", list(x[10]))\n",
    "\n",
    "print(\"t의 갯수 : \", len(t))\n",
    "print(\"t문자의 갯수 : \", len(t[10]))\n",
    "print(\"t를 문자로 나눈 것 : \", list(t[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a09aa76-0560-4dae-a3ab-926832033360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x 모양 :  (50000, 7)\n",
      "10번째 라벨값 :  [ 1  8 11  8  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "#x 문자들을 라벨링\n",
    "vector_x = []\n",
    "for i in range(len(x)) :\n",
    "    temp = list(x[i]) #str을 문자단위로 끊은 list로 바꾸기\n",
    "    vector = custom.word_vectorize(temp, num_dict)\n",
    "    vector_x.append(vector)\n",
    "\n",
    "vector_x = np.array(vector_x)\n",
    "print(\"x 모양 : \", vector_x.shape)\n",
    "print(\"10번째 라벨값 : \", vector_x[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9144ca76-8aab-4a85-a70c-5400e7ce2ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t 모양 :  (50000, 4)\n",
      "10번째 라벨값 :  [2 6 0 0]\n"
     ]
    }
   ],
   "source": [
    "#t 문자들을 라벨링 (출력값 y와 비교하기 위해서) \n",
    "vector_t = []\n",
    "for i in range(len(t)) :\n",
    "    temp = list(t[i])\n",
    "    vector = custom.word_vectorize(temp, num_dict)\n",
    "    vector_t.append(vector)\n",
    "\n",
    "vector_t = np.array(vector_t)\n",
    "print(\"t 모양 : \", vector_t.shape)\n",
    "print(\"10번째 라벨값 : \", vector_t[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8967e8ef-9548-4942-b555-acd47f4de879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# 전처리한 데이터 x, t를 DataLoader에 싣기\n",
    "# train, test 나누겠습니다. 앞의 40000개는 train, 뒤의 10000개는 test\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "s = 40000\n",
    "\n",
    "tensor_x = torch.tensor(vector_x, dtype = torch.long, device = device)\n",
    "tensor_t = torch.tensor(vector_t, dtype = torch.long, device = device)\n",
    "\n",
    "train_zip_list = list(zip(tensor_x[:s], tensor_t[:s]))\n",
    "test_zip_list = list(zip(tensor_x[s:], tensor_t[s:]))\n",
    "\n",
    "train_dataloader = DataLoader(train_zip_list, batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(test_zip_list, batch_size=1000, shuffle=False)\n",
    "\n",
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebcddf0d-a2ee-4a1c-be54-b30bacdd43a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | loss 1.564534757733345 | acc 0.468075 | cnt 0\n",
      "epoch 2 | loss 1.143991583585739 | acc 0.534325 | cnt 0\n",
      "epoch 3 | loss 1.024682889431715 | acc 0.574875 | cnt 0\n",
      "epoch 4 | loss 0.9478731869161129 | acc 0.571775 | cnt 1\n",
      "epoch 5 | loss 0.8979005862772464 | acc 0.6111 | cnt 0\n",
      "epoch 6 | loss 0.8540154963731765 | acc 0.60755 | cnt 1\n",
      "epoch 7 | loss 0.8161561946570873 | acc 0.62035 | cnt 0\n",
      "epoch 8 | loss 0.7720776091516018 | acc 0.65405 | cnt 0\n",
      "epoch 9 | loss 0.6958055786788464 | acc 0.68155 | cnt 0\n",
      "epoch 10 | loss 0.6158981820940972 | acc 0.7133 | cnt 0\n",
      "epoch 11 | loss 0.548891538232565 | acc 0.744 | cnt 0\n",
      "epoch 12 | loss 0.5048222616314888 | acc 0.7627 | cnt 0\n",
      "epoch 13 | loss 0.46004723213613036 | acc 0.769575 | cnt 0\n",
      "epoch 14 | loss 0.4306331096589565 | acc 0.798475 | cnt 0\n",
      "epoch 15 | loss 0.3958446746319532 | acc 0.7966 | cnt 1\n",
      "epoch 16 | loss 0.36850445091724393 | acc 0.8129 | cnt 0\n",
      "epoch 17 | loss 0.3467323074489832 | acc 0.8191 | cnt 0\n",
      "epoch 18 | loss 0.3302297282963991 | acc 0.832775 | cnt 0\n",
      "epoch 19 | loss 0.30263708613812923 | acc 0.840775 | cnt 0\n",
      "epoch 20 | loss 0.2911795987188816 | acc 0.848775 | cnt 0\n",
      "epoch 21 | loss 0.2760118280723691 | acc 0.86605 | cnt 0\n",
      "epoch 22 | loss 0.26171016953885556 | acc 0.858725 | cnt 1\n",
      "epoch 23 | loss 0.25524791304022076 | acc 0.8681 | cnt 0\n",
      "epoch 24 | loss 0.2434176155924797 | acc 0.873375 | cnt 0\n",
      "epoch 25 | loss 0.2356288032606244 | acc 0.877425 | cnt 0\n",
      "epoch 26 | loss 0.2158915200829506 | acc 0.8959 | cnt 0\n",
      "epoch 27 | loss 0.21201384644955396 | acc 0.897925 | cnt 0\n",
      "epoch 28 | loss 0.20358662867918612 | acc 0.8832 | cnt 1\n",
      "epoch 29 | loss 0.20293076928704978 | acc 0.896375 | cnt 2\n",
      "epoch 30 | loss 0.192389117218554 | acc 0.903475 | cnt 0\n",
      "epoch 31 | loss 0.19210556492209435 | acc 0.908825 | cnt 0\n",
      "epoch 32 | loss 0.176741833537817 | acc 0.909575 | cnt 0\n",
      "epoch 33 | loss 0.17288811068981885 | acc 0.9115 | cnt 0\n",
      "epoch 34 | loss 0.17277189038693905 | acc 0.91185 | cnt 0\n",
      "epoch 35 | loss 0.17131952265277506 | acc 0.8987 | cnt 1\n",
      "epoch 36 | loss 0.16687030287459492 | acc 0.90625 | cnt 2\n",
      "epoch 37 | loss 0.15997386129572988 | acc 0.910025 | cnt 3\n",
      "epoch 38 | loss 0.15602277105674148 | acc 0.9234 | cnt 0\n",
      "epoch 39 | loss 0.15060873178765177 | acc 0.937425 | cnt 0\n",
      "epoch 40 | loss 0.1510551193356514 | acc 0.911825 | cnt 1\n",
      "epoch 41 | loss 0.14680331749841571 | acc 0.915425 | cnt 2\n",
      "epoch 42 | loss 0.14992595253512264 | acc 0.92675 | cnt 3\n",
      "epoch 43 | loss 0.14791535375639797 | acc 0.926825 | cnt 4\n",
      "epoch 44 | loss 0.13454996280372142 | acc 0.931475 | cnt 5\n",
      "train stopped\n"
     ]
    }
   ],
   "source": [
    "#AI 만들기\n",
    "# x,t 입력은 데이터로더에서 완료\n",
    "\n",
    "# 함수들 만들기\n",
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, embedding_vector) :\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_vector, freeze = True, padding_idx = 0) # 라벨값을 벡터로 바꾸는 Embedding 함수\n",
    "        self.rnn = nn.LSTM(embedding_vector.shape[1], embedding_vector.shape[1], batch_first = True, bidirectional = True) #임베딩 열 크기를 받아서 임베딩 열 크기를 출력\n",
    "    def forward(self, x) :\n",
    "        x = self.embedding(x)\n",
    "        y, hc = self.rnn(x)\n",
    "        return y, hc\n",
    "\n",
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, embedding_vector) :\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_vector, freeze = True, padding_idx = 0)\n",
    "        self.rnn = nn.LSTM(embedding_vector.shape[1] * 3, embedding_vector.shape[1], batch_first = True, bidirectional = True)\n",
    "        self.f = nn.Linear(embedding_vector.shape[1] * 4, embedding_vector.shape[0]) #RNN을 계산하고 난 뒤 벡터값을 다시 라벨값으로 바꾸는 용도, 단어 출력 용도\n",
    "        self.encoder_h_context = None #encoder_h 가공값 저장할 변수\n",
    "    def forward(self, encoder_output, encoder_hc, t = None) : #문장 여러개 처리하기 위해서 encoder_output.shape가 필요함\n",
    "        #decoder 학습 방법 2가지 (greedy, teaching_force)\n",
    "        #encoder_h 가공\n",
    "        encoder_h_forward = encoder_hc[0][0:1,:,:]\n",
    "        encoder_h_backward = encoder_hc[0][1:2,:,:] #encoder의 h값 반으로 쪼개기\n",
    "        self.encoder_h_context = torch.concat([encoder_h_forward, encoder_h_backward], dim = -1).transpose(0,1) #peeky algorithm, 적용하기 위한 변수\n",
    "        \n",
    "        batch_size = encoder_output.shape[0] #문장 갯수 정하기 = encoder에서 처리한 문장 갯수\n",
    "        decoder_input = torch.zeros(batch_size, 1).type(torch.long).to(encoder_output.device) #처음 넣는 단어 (0번째 단어인 padding)\n",
    "        decoder_hc = encoder_hc\n",
    "        decoder_output_list = [] #단어들 모아서 문장으로 만들어 출력\n",
    "\n",
    "        for i in range(4) : #t의 문자갯수만큼 반복\n",
    "            decoder_output, decoder_hc = self.forward_cal(decoder_input, decoder_hc)\n",
    "            decoder_output_list.append(decoder_output)\n",
    "\n",
    "            if t is None : #greedy (t가 없음)\n",
    "                decoder_input = decoder_output.argmax(dim = -1).detach()\n",
    "            else : #teaching-force (t가 있음)\n",
    "                decoder_input = t[:, i:i+1]\n",
    "\n",
    "        decoder_output_list = torch.cat(decoder_output_list, dim = 1) #for 문 종료 후 list로 모아놓은 tensor들을 tensor로 합치는 작업\n",
    "        return decoder_output_list, decoder_hc, None\n",
    "        \n",
    "    def forward_cal(self, x, h) : #실제 신경망 계산 함수\n",
    "        x = self.embedding(x) #라벨을 벡터로\n",
    "        x = torch.concat([self.encoder_h_context, x], dim = -1) #peeky algorithm 적용\n",
    "        output, h = self.rnn(x, h) #처음 h값을 지정하고 싶으면 x 뒤에 h값 입력하세요\n",
    "        output = torch.concat([self.encoder_h_context, output], dim = -1) #peeky algorithm 적용\n",
    "        output = self.f(output) #벡터 계산값을 다시 라벨값으로 변환\n",
    "        return output, h\n",
    "\n",
    "encoder = Encoder(embedding_tensor).to(device)\n",
    "decoder = Decoder(embedding_tensor).to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr = 0.02)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr = 0.02)\n",
    "epoch = 100\n",
    "prev_acc = 0\n",
    "acc_cnt = 0\n",
    "\n",
    "for e in range(epoch) :\n",
    "    encoder.train()\n",
    "    decoder.train() #드롭아웃 활성화\n",
    "    loss_sum = 0\n",
    "    for x, t in train_dataloader :\n",
    "# y = F(x)\n",
    "        encoder_output, encoder_h = encoder(x)\n",
    "        y, h, _ = decoder(encoder_output, encoder_h, t)\n",
    "# y, t 비교\n",
    "        loss = loss_function(y.reshape(-1, y.shape[-1]), t.reshape(-1)) #cross entropy loss 계산 위해서 3차원을 2차원으로 \n",
    "        loss_sum += loss.item()\n",
    "# F(x) 수정\n",
    "        loss.backward()\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "    loss_sum /= len(train_dataloader)\n",
    "# 중간 acc 점검, earlystopper 구현\n",
    "    encoder.eval()\n",
    "    decoder.eval() #드롭아웃 꺼주기\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, t in test_dataloader :\n",
    "        encoder_output, encoder_h = encoder(x)\n",
    "        y, h, _ = decoder(encoder_output, encoder_h, None) #성능 평가기 때문에 t 입력 안함\n",
    "        # for i in range(len(x)) :\n",
    "        #     for j in range(4) :\n",
    "        #         if (y[i][j].argmax() == t[i][j]) : #단어 하나하나씩 비교\n",
    "        #             correct += 1\n",
    "        correct += (y.argmax(dim = -1) == t).sum().item()\n",
    "        total += len(x) * 4\n",
    "    acc = correct / total\n",
    "\n",
    "    if acc <= prev_acc :\n",
    "        acc_cnt += 1\n",
    "    else :\n",
    "        acc_cnt = 0\n",
    "        prev_acc = acc\n",
    "    print(f\"epoch {e+1} | loss {loss_sum} | acc {acc} | cnt {acc_cnt}\")\n",
    "    if acc_cnt >= 5:\n",
    "        print(\"train stopped\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c9287e7-bf41-407f-a7bb-0662ae70286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#신경망 저장\n",
    "\n",
    "torch.save(encoder.to(\"cpu\"), \"num_encoder.pt\")\n",
    "torch.save(decoder.to(\"cpu\"), \"num_decoder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b22ad-2013-44e5-9de4-d09db71d0b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
