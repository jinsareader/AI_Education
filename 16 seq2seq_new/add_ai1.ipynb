{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e79d4a3-f6f0-4dcd-89fa-423105abc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data 전처리\n",
    "## 숫자 벡터 사전 만들기 \n",
    "## 데이터 불러오기\n",
    "## 데이터 x t로 나누기\n",
    "## 데이터 라벨링=정수로 바꾸기(신경망 안에 벡터 사전이 있을 때) or 벡터화=1차원 배열(신경망 안에 벡터 사진이 없을 때)\n",
    "### 참고로 우리는 아직 <sos>와 <eos>를 쓰지 않을 겁니다.\n",
    "\n",
    "# AI 만들기\n",
    "## AI 학습\n",
    "### DataLoader에 싣기 (x, t 입력)\n",
    "### Encoder와 Decoder 구현 하기 (함수 만들기)\n",
    "### y = F(x) (순전파)\n",
    "### 손실함수\n",
    "### 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed06b3b-e8d2-4f40-be6d-e6768a3c271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 import\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"\"), \"..\"))\n",
    "import custom\n",
    "import numpy as np\n",
    "\n",
    "# Data 전처리 모듈 : custom과 numpy만으로 가능함\n",
    "from tqdm import tqdm #자료가 많으니 진행상황은 봅시다\n",
    "\n",
    "# AI 만들기 모듈\n",
    "import torch\n",
    "import torch.nn as nn #클래스 형식의 신경망 함수\n",
    "import torch.nn.functional as F #함수 형식의 신경망 함수\n",
    "from torch.utils.data import DataLoader #데이터 batch 나눠주는 역할의 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b374ef61-4566-42a8-bfa1-597ddcb85d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "(12, 11)\n",
      "{' ': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '0': 10, '+': 11}\n"
     ]
    }
   ],
   "source": [
    "## 숫자 벡터 사전 만들기\n",
    "## ' ',1,2,3,4,5,6,7,8,9,0,+ : 12개\n",
    "## ' '(pad 문자) 는 0행렬\n",
    "## 나머지 11개는 원 핫 인코딩\n",
    "\n",
    "#벡터 값 만들기\n",
    "vector_list = np.zeros((1,11)) # np.array의 백터 값 저장할 pad값만 저장된 행렬\n",
    "vector_list = np.append(vector_list, np.eye(11), axis = 0) # 대각선 값은 1이고 나머지는 0인 정사각 행렬\n",
    "print(vector_list)\n",
    "print(vector_list.shape)\n",
    "\n",
    "#숫자-라벨링 사전\n",
    "num_dict = {}\n",
    "num_dict[' '] = 0\n",
    "\n",
    "for i in range(1,10) : #1~9까지 반복문으로 넣기\n",
    "    num_dict[str(i)] = i\n",
    "num_dict['0'] = 10\n",
    "num_dict['+'] = 11\n",
    "print(num_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29dd6a5a-058b-4305-b208-efc3cb2fd7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "18+8   _26  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 데이터 불러오기\n",
    "file_name = 'addition.txt'\n",
    "with open(file_name, mode = \"r\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "print(len(data))\n",
    "print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244718da-56d7-4bfa-b0f6-6de4d45eb3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x의 갯수 : 50000\n",
      "t의 갯수 : 50000\n",
      "10번째 x : 18+8   \n",
      "10번째 t : 26  \n",
      "10번째 x 길이 : 7\n",
      "10번째 t 길이 : 4\n"
     ]
    }
   ],
   "source": [
    "## 데이터 x, t로 나누기\n",
    "data_x = []\n",
    "data_t = []\n",
    "\n",
    "for d in data :\n",
    "    d = d.split(\"_\")\n",
    "    data_x.append(d[0])\n",
    "    data_t.append(d[1].replace('\\n','')) #개행문자 제거\n",
    "\n",
    "print(\"x의 갯수 :\",len(data_x))\n",
    "print(\"t의 갯수 :\",len(data_t))\n",
    "print(\"10번째 x :\",data_x[10])\n",
    "print(\"10번째 t :\",data_t[10])\n",
    "print(\"10번째 x 길이 :\",len(data_x[10]))\n",
    "print(\"10번째 t 길이 :\",len(data_t[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e9a3d9-7065-44d2-b943-a7015f121c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 7)\n",
      "(50000, 4)\n"
     ]
    }
   ],
   "source": [
    "## 데이터 라벨링=정수로 바꾸기(신경망 안에 벡터 사전이 있을 때) or 벡터화=1차원 배열(신경망 안에 벡터 사진이 없을 때)\n",
    "## 라벨링 하겠습니다 (= 신경망 안에 벡터 list 넣겠습니다)\n",
    "\n",
    "for i in range(len(data_x)) :\n",
    "    temp = data_x[i]\n",
    "    temp = list(temp) #문장을 문자 하나하나씩 끊어서 list화 #역산 하려면 \"\".join() 쓰면 됩니다\n",
    "    vector = custom.word_vectorize(temp, num_dict, 7)\n",
    "    data_x[i] = vector\n",
    "data_x = np.array(data_x)\n",
    "\n",
    "for i in range(len(data_t)) :\n",
    "    temp = data_t[i]\n",
    "    temp = list(temp)\n",
    "    vector = custom.word_vectorize(temp, num_dict, 4)\n",
    "    data_t[i] = vector\n",
    "data_t = np.array(data_t)\n",
    "\n",
    "print(data_x.shape)\n",
    "print(data_t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45efbc64-1232-41e0-bac8-f18ae3f160c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "## AI 학습\n",
    "### DataLoader에 싣기 (x, t 입력)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tensor_x = torch.tensor(data_x, dtype = torch.long, device = device)\n",
    "tensor_t = torch.tensor(data_t, dtype = torch.long, device = device)\n",
    "\n",
    "### 앞의 40000번째 까지는 train, 뒤의 10000개는 test(val)\n",
    "s = 40000\n",
    "train_zip = list(zip(tensor_x[:s], tensor_t[:s])) \n",
    "test_zip = list(zip(tensor_x[s:], tensor_t[s:]))\n",
    "\n",
    "train_dataloader = DataLoader(train_zip, batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(test_zip, batch_size=1000, shuffle=False)\n",
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea636c0e-8cf4-4f5f-9c6f-1515f670b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder와 Decoder 구현 하기 (함수 만들기)\n",
    "\n",
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, vector) :\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(vector, freeze = True, padding_idx=0) # 벡터화 함수, b값이 없는 선형함수, 차원 = (단어갯수, 벡터갯수)\n",
    "        #.from_pretrained : W값을 해당 벡터로 만들겠다\n",
    "        self.rnn = nn.RNN(vector.shape[1], vector.shape[1], batch_first = True)# RNN 함수\n",
    "    def forward(self, x) :\n",
    "        x = self.embedding(x)\n",
    "        y, h = self.rnn(x)\n",
    "        return y, h\n",
    "\n",
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, vector, max_len) :\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(vector, freeze = True, padding_idx=0) #벡터화 함수\n",
    "        self.rnn = nn.RNN(vector.shape[1], vector.shape[1], batch_first = True) # RNN 함수\n",
    "        self.f = nn.Linear(vector.shape[1], vector.shape[0]) #벡터 값을 다시 단어 라벨값으로 역산하는 함수\n",
    "        self.max_len = max_len #문장 단어의 최대 갯수\n",
    "    def forward(self, encoder_output, encoder_h, t = None) :\n",
    "        decoder_y_list = [] #출력값\n",
    "\n",
    "        decoder_x = torch.zeros((encoder_output.shape[0],1)).type(torch.long).to(encoder_output.device) #첫번째 입력값, 라벨값이기 때문에 차원 = (문장 갯수,1), 문장 갯수는 encoder_output에서 가져옵니다\n",
    "        #.type(torch.long) 은 형변환, .to(encoder_output.device) 은 기기 배정\n",
    "        decoder_h = encoder_h #첫번째 h값       \n",
    "\n",
    "        for i in range(self.max_len) :\n",
    "            decoder_y, decoder_h = self.forward_cal(decoder_x, decoder_h) # RNN 계산 한블록 합니다.\n",
    "\n",
    "            if t is not None : #teaching force, t가 존재할 때\n",
    "                decoder_x = t[:,i:i+1]  # i번째 t의 단어 가져옴\n",
    "            else : # greedy, t가 존재하지 않을 때\n",
    "                decoder_x = torch.argmax(decoder_y, dim = -1).detach() #출력 값 중에 가장 큰값(라벨값) 가져오고, 해당 값은 미분 대상이 아님 (.detach())\n",
    "\n",
    "            decoder_y_list.append(decoder_y) #출력값에 단어 하나하나 씩 저장\n",
    "\n",
    "        decoder_y_list = torch.cat(decoder_y_list, dim = 1) #list를 tensor로 묶기 위해서 cat 함수 쓰는 것 decoder_y.shape = (문장 갯수, 단어 갯수(1개), 벡터 갯수)\n",
    "        \n",
    "        return decoder_y_list, decoder_h, None # decoder layer 2개 이상 묶어서 쓰기 위해서 h값과, attention값도 return하기 위해 3개의 return 값\n",
    "    def forward_cal(self, x, h) : #rnn 블록하나 계산하는 함수\n",
    "        x = self.embedding(x) # 벡터로 변환하고\n",
    "        x, h = self.rnn(x, h) # rnn 계산, h값은 이전 h계산 결과, h값도 따로 입력합니다\n",
    "        x = self.f(x) # 벡터 계산값 역산해서 다시 라벨로 변환\n",
    "        return x, h\n",
    "        \n",
    "class Encoder_n_Decoder(nn.Module) : #encoder와 decoder를 하나로 묶는 새로운 신경망\n",
    "    def __init__(self, encoder, decoder) :\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, x, t = None) :\n",
    "        y, h = self.encoder(x)\n",
    "        y, _, _ = self.decoder(y, h, t)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b67dfe3-14c5-4759-bbc2-130cc55c3865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | loss 1.770693689584732 | acc 0.3726 | cnt 0\n",
      "epoch 2 | loss 1.5339281985163689 | acc 0.42425 | cnt 0\n",
      "epoch 3 | loss 1.4408606171607972 | acc 0.418675 | cnt 1\n",
      "epoch 4 | loss 1.4085644102096557 | acc 0.448825 | cnt 0\n",
      "epoch 5 | loss 1.3833784195780754 | acc 0.4573 | cnt 0\n",
      "epoch 6 | loss 1.3720580649375915 | acc 0.471425 | cnt 0\n",
      "epoch 7 | loss 1.3702025324106217 | acc 0.451775 | cnt 1\n",
      "epoch 8 | loss 1.3585444104671478 | acc 0.4034 | cnt 2\n",
      "epoch 9 | loss 1.3562321108579636 | acc 0.438025 | cnt 3\n",
      "epoch 10 | loss 1.3533389818668367 | acc 0.44665 | cnt 4\n",
      "epoch 11 | loss 1.3429065403342246 | acc 0.469125 | cnt 5\n",
      "train stopped\n"
     ]
    }
   ],
   "source": [
    "tensor_vector = torch.tensor(vector_list, dtype = torch.float)\n",
    "encoder = Encoder(tensor_vector).to(device)\n",
    "decoder = Decoder(tensor_vector, 4).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "encoder_optim = torch.optim.Adam(encoder.parameters(), lr = 0.02)\n",
    "decoder_optim = torch.optim.Adam(decoder.parameters(), lr = 0.02)\n",
    "\n",
    "epoch = 100\n",
    "prev_acc = 0\n",
    "cnt = 0\n",
    "\n",
    "for e in range(epoch) :\n",
    "    loss_sum = 0\n",
    "    encoder.train() #dropout 켜주기\n",
    "    decoder.train()\n",
    "    for x, t in train_dataloader :\n",
    "### y = F(x) (순전파)\n",
    "        y, h = encoder(x)\n",
    "        y, _, _ = decoder(y, h)\n",
    "### 손실함수\n",
    "        loss = loss_func(y.reshape(-1, y.shape[-1]) , t.reshape(-1)) #3차원을 2차원으로 펴주기\n",
    "        loss_sum += loss.item()\n",
    "### 역전파\n",
    "        loss.backward()\n",
    "        decoder_optim.step()\n",
    "        encoder_optim.step()\n",
    "        decoder_optim.zero_grad()\n",
    "        encoder_optim.zero_grad()\n",
    "    loss_sum /= len(train_dataloader) #평균 구하기\n",
    "### 중간 점검\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    encoder.eval() #dropout 꺼주기\n",
    "    decoder.eval()\n",
    "    for x, t in test_dataloader :\n",
    "        with torch.no_grad() :\n",
    "            y, h = encoder(x)\n",
    "            y, _, _ = decoder(y, h)\n",
    "            correct += (y.argmax(dim = -1) == t).sum().item()\n",
    "        total += len(x) * 4\n",
    "    acc = correct / total\n",
    "### earlystopper\n",
    "    if acc <= prev_acc :\n",
    "        cnt += 1\n",
    "    else :\n",
    "        cnt = 0\n",
    "        prev_acc = acc\n",
    "        torch.save(encoder, 'encoder1.pt') #중간 저장\n",
    "        torch.save(decoder, 'decoder1.pt')\n",
    "    print(f\"epoch {e+1} | loss {loss_sum} | acc {acc} | cnt {cnt}\")\n",
    "    if cnt >= 5 :\n",
    "        print(\"train stopped\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ba0a399-07bb-49aa-abf4-48c626877899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder_n_Decoder(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(12, 11, padding_idx=0)\n",
      "    (rnn): RNN(11, 11, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(12, 11, padding_idx=0)\n",
      "    (rnn): RNN(11, 11, batch_first=True)\n",
      "    (f): Linear(in_features=11, out_features=12, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# onnx로 변환하기 쉽게 하기 위해 다시 불러와서 cpu로 변환 한 다음에 하나로 묶는 작업\n",
    "\n",
    "encoder = torch.load('encoder1.pt', weights_only=False, map_location='cpu') #cpu로 불러오기 \n",
    "decoder = torch.load('decoder1.pt', weights_only=False, map_location='cpu')\n",
    "\n",
    "F = Encoder_n_Decoder(encoder, decoder)\n",
    "print(F)\n",
    "print(F.state_dict()['encoder.embedding.weight']) #.state_dict() : 신경망 내의 W, b 값 불러오는 함수\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9d1727c-3dad-4901-ad9f-02f21de551c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 묶은 신경망을 다시 저장\n",
    "\n",
    "torch.save(F, \"add_ai.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f1cce-5e92-46b1-bf09-0d04d6d31ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
