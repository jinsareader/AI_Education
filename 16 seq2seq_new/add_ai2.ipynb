{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e79d4a3-f6f0-4dcd-89fa-423105abc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data 전처리\n",
    "## 숫자 벡터 사전 만들기 \n",
    "## 데이터 불러오기\n",
    "## 데이터 x t로 나누기\n",
    "## 데이터 라벨링=정수로 바꾸기(신경망 안에 벡터 사전이 있을 때) or 벡터화=1차원 배열(신경망 안에 벡터 사진이 없을 때)\n",
    "### 참고로 우리는 아직 <sos>와 <eos>를 쓰지 않을 겁니다.\n",
    "\n",
    "# AI 만들기\n",
    "## AI 학습\n",
    "### DataLoader에 싣기 (x, t 입력)\n",
    "### Encoder와 Decoder 구현 하기 (함수 만들기)\n",
    "### y = F(x) (순전파)\n",
    "### 손실함수\n",
    "### 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed06b3b-e8d2-4f40-be6d-e6768a3c271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 import\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"\"), \"..\"))\n",
    "import custom\n",
    "import numpy as np\n",
    "\n",
    "# Data 전처리 모듈 : custom과 numpy만으로 가능함\n",
    "from tqdm import tqdm #자료가 많으니 진행상황은 봅시다\n",
    "\n",
    "# AI 만들기 모듈\n",
    "import torch\n",
    "import torch.nn as nn #클래스 형식의 신경망 함수\n",
    "import torch.nn.functional as F #함수 형식의 신경망 함수\n",
    "from torch.utils.data import DataLoader #데이터 batch 나눠주는 역할의 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b374ef61-4566-42a8-bfa1-597ddcb85d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "(12, 11)\n",
      "{' ': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '0': 10, '+': 11}\n"
     ]
    }
   ],
   "source": [
    "## 숫자 벡터 사전 만들기\n",
    "## ' ',1,2,3,4,5,6,7,8,9,0,+ : 12개\n",
    "## ' '(pad 문자) 는 0행렬\n",
    "## 나머지 11개는 원 핫 인코딩\n",
    "\n",
    "#벡터 값 만들기\n",
    "vector_list = np.zeros((1,11)) # np.array의 백터 값 저장할 pad값만 저장된 행렬\n",
    "vector_list = np.append(vector_list, np.eye(11), axis = 0) # 대각선 값은 1이고 나머지는 0인 정사각 행렬\n",
    "print(vector_list)\n",
    "print(vector_list.shape)\n",
    "\n",
    "#숫자-라벨링 사전\n",
    "num_dict = {}\n",
    "num_dict[' '] = 0\n",
    "\n",
    "for i in range(1,10) : #1~9까지 반복문으로 넣기\n",
    "    num_dict[str(i)] = i\n",
    "num_dict['0'] = 10\n",
    "num_dict['+'] = 11\n",
    "print(num_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29dd6a5a-058b-4305-b208-efc3cb2fd7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "18+8   _26  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 데이터 불러오기\n",
    "file_name = 'addition.txt'\n",
    "with open(file_name, mode = \"r\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "print(len(data))\n",
    "print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244718da-56d7-4bfa-b0f6-6de4d45eb3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x의 갯수 : 50000\n",
      "t의 갯수 : 50000\n",
      "10번째 x : 18+8   \n",
      "10번째 t : 26  \n",
      "10번째 x 길이 : 7\n",
      "10번째 t 길이 : 4\n"
     ]
    }
   ],
   "source": [
    "## 데이터 x, t로 나누기\n",
    "data_x = []\n",
    "data_t = []\n",
    "\n",
    "for d in data :\n",
    "    d = d.split(\"_\")\n",
    "    data_x.append(d[0])\n",
    "    data_t.append(d[1].replace('\\n','')) #개행문자 제거\n",
    "\n",
    "print(\"x의 갯수 :\",len(data_x))\n",
    "print(\"t의 갯수 :\",len(data_t))\n",
    "print(\"10번째 x :\",data_x[10])\n",
    "print(\"10번째 t :\",data_t[10])\n",
    "print(\"10번째 x 길이 :\",len(data_x[10]))\n",
    "print(\"10번째 t 길이 :\",len(data_t[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e9a3d9-7065-44d2-b943-a7015f121c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 7)\n",
      "(50000, 4)\n"
     ]
    }
   ],
   "source": [
    "## 데이터 라벨링=정수로 바꾸기(신경망 안에 벡터 사전이 있을 때) or 벡터화=1차원 배열(신경망 안에 벡터 사진이 없을 때)\n",
    "## 라벨링 하겠습니다 (= 신경망 안에 벡터 list 넣겠습니다)\n",
    "\n",
    "for i in range(len(data_x)) :\n",
    "    temp = data_x[i]\n",
    "    temp = list(temp) #문장을 문자 하나하나씩 끊어서 list화 #역산 하려면 \"\".join() 쓰면 됩니다\n",
    "    vector = custom.word_vectorize(temp, num_dict, 7)\n",
    "    data_x[i] = vector\n",
    "data_x = np.array(data_x)\n",
    "\n",
    "for i in range(len(data_t)) :\n",
    "    temp = data_t[i]\n",
    "    temp = list(temp)\n",
    "    vector = custom.word_vectorize(temp, num_dict, 4)\n",
    "    data_t[i] = vector\n",
    "data_t = np.array(data_t)\n",
    "\n",
    "print(data_x.shape)\n",
    "print(data_t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45efbc64-1232-41e0-bac8-f18ae3f160c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "## AI 학습\n",
    "### DataLoader에 싣기 (x, t 입력)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tensor_x = torch.tensor(data_x, dtype = torch.long, device = device)\n",
    "tensor_t = torch.tensor(data_t, dtype = torch.long, device = device)\n",
    "\n",
    "### 앞의 40000번째 까지는 train, 뒤의 10000개는 test(val)\n",
    "s = 40000\n",
    "train_zip = list(zip(tensor_x[:s], tensor_t[:s])) \n",
    "test_zip = list(zip(tensor_x[s:], tensor_t[s:]))\n",
    "\n",
    "train_dataloader = DataLoader(train_zip, batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(test_zip, batch_size=1000, shuffle=False)\n",
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea636c0e-8cf4-4f5f-9c6f-1515f670b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder와 Decoder 구현 하기 (함수 만들기)\n",
    "\n",
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, vector) :\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(vector, freeze = True, padding_idx=0) # 벡터화 함수, b값이 없는 선형함수, 차원 = (단어갯수, 벡터갯수)\n",
    "        #.from_pretrained : W값을 해당 벡터로 만들겠다\n",
    "        self.rnn = nn.LSTM(vector.shape[1], vector.shape[1], batch_first = True)# RNN 함수\n",
    "    def forward(self, x) :\n",
    "        x = self.embedding(x)\n",
    "        y, hc = self.rnn(x) #LSTM은 hc\n",
    "        return y, hc\n",
    "\n",
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, vector, max_len) :\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(vector, freeze = True, padding_idx=0) #벡터화 함수\n",
    "        self.rnn = nn.LSTM(vector.shape[1], vector.shape[1], batch_first = True) # RNN 함수\n",
    "        self.f = nn.Linear(vector.shape[1], vector.shape[0]) #벡터 값을 다시 단어 라벨값으로 역산하는 함수\n",
    "        self.max_len = max_len #문장 단어의 최대 갯수\n",
    "    def forward(self, encoder_output, encoder_hc, t = None) :\n",
    "        decoder_y_list = [] #출력값\n",
    "\n",
    "        decoder_x = torch.zeros((encoder_output.shape[0],1)).type(torch.long).to(encoder_output.device) #첫번째 입력값, 라벨값이기 때문에 차원 = (문장 갯수,1), 문장 갯수는 encoder_output에서 가져옵니다\n",
    "        #.type(torch.long) 은 형변환, .to(encoder_output.device) 은 기기 배정\n",
    "        decoder_hc = encoder_hc #첫번째 hc값       \n",
    "\n",
    "        for i in range(self.max_len) :\n",
    "            decoder_y, decoder_hc = self.forward_cal(decoder_x, decoder_hc) # RNN 계산 한블록 합니다.\n",
    "\n",
    "            if t is not None : #teaching force, t가 존재할 때\n",
    "                decoder_x = t[:,i:i+1]  # i번째 t의 단어 가져옴\n",
    "            else : # greedy, t가 존재하지 않을 때\n",
    "                decoder_x = torch.argmax(decoder_y, dim = -1).detach() #출력 값 중에 가장 큰값(라벨값) 가져오고, 해당 값은 미분 대상이 아님 (.detach())\n",
    "\n",
    "            decoder_y_list.append(decoder_y) #출력값에 단어 하나하나 씩 저장\n",
    "\n",
    "        decoder_y_list = torch.cat(decoder_y_list, dim = 1) #list를 tensor로 묶기 위해서 cat 함수 쓰는 것 decoder_y.shape = (문장 갯수, 단어 갯수(1개), 벡터 갯수)\n",
    "        \n",
    "        return decoder_y_list, decoder_hc, None # decoder layer 2개 이상 묶어서 쓰기 위해서 hc값과, attention값도 return하기 위해 3개의 return 값\n",
    "    def forward_cal(self, x, hc) : #rnn 블록하나 계산하는 함수\n",
    "        x = self.embedding(x) # 벡터로 변환하고\n",
    "        x, hc = self.rnn(x, hc) # rnn 계산, hc값은 이전 hc계산 결과, hc값도 따로 입력합니다\n",
    "        x = self.f(x) # 벡터 계산값 역산해서 다시 라벨로 변환\n",
    "        return x, hc\n",
    "        \n",
    "class Encoder_n_Decoder(nn.Module) : #encoder와 decoder를 하나로 묶는 새로운 신경망\n",
    "    def __init__(self, encoder, decoder) :\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, x, t = None) :\n",
    "        y, hc = self.encoder(x)\n",
    "        y, _, _ = self.decoder(y, hc, t)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b67dfe3-14c5-4759-bbc2-130cc55c3865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | loss 1.7991171342134475 | acc 0.348175 | cnt 0\n",
      "epoch 2 | loss 1.579795319736004 | acc 0.403425 | cnt 0\n",
      "epoch 3 | loss 1.4599952599406243 | acc 0.451525 | cnt 0\n",
      "epoch 4 | loss 1.3737275674939156 | acc 0.480975 | cnt 0\n",
      "epoch 5 | loss 1.3241031104326249 | acc 0.488375 | cnt 0\n",
      "epoch 6 | loss 1.2962999826669692 | acc 0.5 | cnt 0\n",
      "epoch 7 | loss 1.2722514262795448 | acc 0.49355 | cnt 1\n",
      "epoch 8 | loss 1.2533858299255372 | acc 0.513575 | cnt 0\n",
      "epoch 9 | loss 1.2377476438879966 | acc 0.50825 | cnt 1\n",
      "epoch 10 | loss 1.2281593346595765 | acc 0.515925 | cnt 0\n",
      "epoch 11 | loss 1.2115448290109634 | acc 0.51775 | cnt 0\n",
      "epoch 12 | loss 1.2045693236589432 | acc 0.533725 | cnt 0\n",
      "epoch 13 | loss 1.1939394670724868 | acc 0.5162 | cnt 1\n",
      "epoch 14 | loss 1.1890200120210648 | acc 0.521975 | cnt 2\n",
      "epoch 15 | loss 1.17507807046175 | acc 0.5217 | cnt 3\n",
      "epoch 16 | loss 1.1679693844914436 | acc 0.528075 | cnt 4\n",
      "epoch 17 | loss 1.16140584141016 | acc 0.54175 | cnt 0\n",
      "epoch 18 | loss 1.1559047532081603 | acc 0.537975 | cnt 1\n",
      "epoch 19 | loss 1.1463766932487487 | acc 0.543275 | cnt 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x, t \u001b[38;5;129;01min\u001b[39;00m train_dataloader :\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m### y = F(x) (순전파)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m         y, hc \u001b[38;5;241m=\u001b[39m encoder(x)\n\u001b[1;32m---> 22\u001b[0m         y, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m### 손실함수\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_func(y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) , t\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m#3차원을 2차원으로 펴주기\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, encoder_output, encoder_hc, t)\u001b[0m\n\u001b[0;32m     32\u001b[0m         decoder_x \u001b[38;5;241m=\u001b[39m t[:,i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# i번째 t의 단어 가져옴\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m : \u001b[38;5;66;03m# greedy, t가 존재하지 않을 때\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m         decoder_x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;66;03m#출력 값 중에 가장 큰값(라벨값) 가져오고, 해당 값은 미분 대상이 아님 (.detach())\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     decoder_y_list\u001b[38;5;241m.\u001b[39mappend(decoder_y) \u001b[38;5;66;03m#출력값에 단어 하나하나 씩 저장\u001b[39;00m\n\u001b[0;32m     38\u001b[0m decoder_y_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(decoder_y_list, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#list를 tensor로 묶기 위해서 cat 함수 쓰는 것 decoder_y.shape = (문장 갯수, 단어 갯수(1개), 벡터 갯수)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "encoder_file_name = 'data/encoder2.pt'\n",
    "decoder_file_name = 'data/decoder2.pt'\n",
    "\n",
    "tensor_vector = torch.tensor(vector_list, dtype = torch.float)\n",
    "encoder = Encoder(tensor_vector).to(device)\n",
    "decoder = Decoder(tensor_vector, 4).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "encoder_optim = torch.optim.Adam(encoder.parameters(), lr = 0.02)\n",
    "decoder_optim = torch.optim.Adam(decoder.parameters(), lr = 0.02)\n",
    "\n",
    "epoch = 100\n",
    "prev_acc = 0\n",
    "cnt = 0\n",
    "\n",
    "for e in range(epoch) :\n",
    "    loss_sum = 0\n",
    "    encoder.train() #dropout 켜주기\n",
    "    decoder.train()\n",
    "    for x, t in train_dataloader :\n",
    "### y = F(x) (순전파)\n",
    "        y, hc = encoder(x)\n",
    "        y, _, _ = decoder(y, hc)\n",
    "### 손실함수\n",
    "        loss = loss_func(y.reshape(-1, y.shape[-1]) , t.reshape(-1)) #3차원을 2차원으로 펴주기\n",
    "        loss_sum += loss.item()\n",
    "### 역전파\n",
    "        loss.backward()\n",
    "        decoder_optim.step()\n",
    "        encoder_optim.step()\n",
    "        decoder_optim.zero_grad()\n",
    "        encoder_optim.zero_grad()\n",
    "    loss_sum /= len(train_dataloader) #평균 구하기\n",
    "### 중간 점검\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    encoder.eval() #dropout 꺼주기\n",
    "    decoder.eval()\n",
    "    for x, t in test_dataloader :\n",
    "        with torch.no_grad() :\n",
    "            y, hc = encoder(x)\n",
    "            y, _, _ = decoder(y, hc)\n",
    "            correct += (y.argmax(dim = -1) == t).sum().item()\n",
    "        total += len(x) * 4\n",
    "    acc = correct / total\n",
    "### earlystopper\n",
    "    if acc <= prev_acc :\n",
    "        cnt += 1\n",
    "    else :\n",
    "        cnt = 0\n",
    "        prev_acc = acc\n",
    "        torch.save(encoder, encoder_file_name) #중간 저장\n",
    "        torch.save(decoder, decoder_file_name)\n",
    "    print(f\"epoch {e+1} | loss {loss_sum} | acc {acc} | cnt {cnt}\")\n",
    "    if cnt >= 5 :\n",
    "        print(\"train stopped\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba0a399-07bb-49aa-abf4-48c626877899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx로 변환하기 쉽게 하기 위해 다시 불러와서 cpu로 변환 한 다음에 하나로 묶는 작업\n",
    "\n",
    "encoder = torch.load(encoder_file_name, weights_only=False, map_location='cpu') #cpu로 불러오기 \n",
    "decoder = torch.load(decoder_file_name, weights_only=False, map_location='cpu')\n",
    "\n",
    "F = Encoder_n_Decoder(encoder, decoder)\n",
    "print(F)\n",
    "print(F.state_dict()['encoder.embedding.weight']) #.state_dict() : 신경망 내의 W, b 값 불러오는 함수\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1727c-3dad-4901-ad9f-02f21de551c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 묶은 신경망을 다시 저장\n",
    "\n",
    "torch.save(F, \"data/add_ai2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f1cce-5e92-46b1-bf09-0d04d6d31ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
