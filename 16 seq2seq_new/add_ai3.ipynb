{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e79d4a3-f6f0-4dcd-89fa-423105abc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data 전처리\n",
    "## 숫자 벡터 사전 만들기 \n",
    "## 데이터 불러오기\n",
    "## 데이터 x t로 나누기\n",
    "## 데이터 라벨링=정수로 바꾸기(신경망 안에 벡터 사전이 있을 때) or 벡터화=1차원 배열(신경망 안에 벡터 사진이 없을 때)\n",
    "### 참고로 우리는 아직 <sos>와 <eos>를 쓰지 않을 겁니다.\n",
    "\n",
    "# AI 만들기\n",
    "## AI 학습\n",
    "### DataLoader에 싣기 (x, t 입력)\n",
    "### Encoder와 Decoder 구현 하기 (함수 만들기)\n",
    "### y = F(x) (순전파)\n",
    "### 손실함수\n",
    "### 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ed06b3b-e8d2-4f40-be6d-e6768a3c271f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;66;03m#자료가 많으니 진행상황은 봅시다\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# AI 만들기 모듈\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;66;03m#클래스 형식의 신경망 함수\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m \u001b[38;5;66;03m#함수 형식의 신경망 함수\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:2151\u001b[0m\n\u001b[0;32m   2135\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2136\u001b[0m \u001b[38;5;66;03m# Import most common subpackages\u001b[39;00m\n\u001b[0;32m   2137\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2142\u001b[0m \n\u001b[0;32m   2143\u001b[0m \u001b[38;5;66;03m# needs to be before import torch.nn as nn to avoid circular dependencies\u001b[39;00m\n\u001b[0;32m   2144\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2145\u001b[0m     enable_grad \u001b[38;5;28;01mas\u001b[39;00m enable_grad,\n\u001b[0;32m   2146\u001b[0m     inference_mode \u001b[38;5;28;01mas\u001b[39;00m inference_mode,\n\u001b[0;32m   2147\u001b[0m     no_grad \u001b[38;5;28;01mas\u001b[39;00m no_grad,\n\u001b[0;32m   2148\u001b[0m     set_grad_enabled \u001b[38;5;28;01mas\u001b[39;00m set_grad_enabled,\n\u001b[0;32m   2149\u001b[0m )\n\u001b[1;32m-> 2151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m   2152\u001b[0m     __config__ \u001b[38;5;28;01mas\u001b[39;00m __config__,\n\u001b[0;32m   2153\u001b[0m     __future__ \u001b[38;5;28;01mas\u001b[39;00m __future__,\n\u001b[0;32m   2154\u001b[0m     _awaits \u001b[38;5;28;01mas\u001b[39;00m _awaits,\n\u001b[0;32m   2155\u001b[0m     accelerator \u001b[38;5;28;01mas\u001b[39;00m accelerator,\n\u001b[0;32m   2156\u001b[0m     autograd \u001b[38;5;28;01mas\u001b[39;00m autograd,\n\u001b[0;32m   2157\u001b[0m     backends \u001b[38;5;28;01mas\u001b[39;00m backends,\n\u001b[0;32m   2158\u001b[0m     cpu \u001b[38;5;28;01mas\u001b[39;00m cpu,\n\u001b[0;32m   2159\u001b[0m     cuda \u001b[38;5;28;01mas\u001b[39;00m cuda,\n\u001b[0;32m   2160\u001b[0m     distributed \u001b[38;5;28;01mas\u001b[39;00m distributed,\n\u001b[0;32m   2161\u001b[0m     distributions \u001b[38;5;28;01mas\u001b[39;00m distributions,\n\u001b[0;32m   2162\u001b[0m     fft \u001b[38;5;28;01mas\u001b[39;00m fft,\n\u001b[0;32m   2163\u001b[0m     futures \u001b[38;5;28;01mas\u001b[39;00m futures,\n\u001b[0;32m   2164\u001b[0m     hub \u001b[38;5;28;01mas\u001b[39;00m hub,\n\u001b[0;32m   2165\u001b[0m     jit \u001b[38;5;28;01mas\u001b[39;00m jit,\n\u001b[0;32m   2166\u001b[0m     linalg \u001b[38;5;28;01mas\u001b[39;00m linalg,\n\u001b[0;32m   2167\u001b[0m     mps \u001b[38;5;28;01mas\u001b[39;00m mps,\n\u001b[0;32m   2168\u001b[0m     mtia \u001b[38;5;28;01mas\u001b[39;00m mtia,\n\u001b[0;32m   2169\u001b[0m     multiprocessing \u001b[38;5;28;01mas\u001b[39;00m multiprocessing,\n\u001b[0;32m   2170\u001b[0m     nested \u001b[38;5;28;01mas\u001b[39;00m nested,\n\u001b[0;32m   2171\u001b[0m     nn \u001b[38;5;28;01mas\u001b[39;00m nn,\n\u001b[0;32m   2172\u001b[0m     optim \u001b[38;5;28;01mas\u001b[39;00m optim,\n\u001b[0;32m   2173\u001b[0m     overrides \u001b[38;5;28;01mas\u001b[39;00m overrides,\n\u001b[0;32m   2174\u001b[0m     profiler \u001b[38;5;28;01mas\u001b[39;00m profiler,\n\u001b[0;32m   2175\u001b[0m     sparse \u001b[38;5;28;01mas\u001b[39;00m sparse,\n\u001b[0;32m   2176\u001b[0m     special \u001b[38;5;28;01mas\u001b[39;00m special,\n\u001b[0;32m   2177\u001b[0m     testing \u001b[38;5;28;01mas\u001b[39;00m testing,\n\u001b[0;32m   2178\u001b[0m     types \u001b[38;5;28;01mas\u001b[39;00m types,\n\u001b[0;32m   2179\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[0;32m   2180\u001b[0m     xpu \u001b[38;5;28;01mas\u001b[39;00m xpu,\n\u001b[0;32m   2181\u001b[0m )\n\u001b[0;32m   2182\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msignal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m windows \u001b[38;5;28;01mas\u001b[39;00m windows\n\u001b[0;32m   2185\u001b[0m \u001b[38;5;66;03m# Quantized, sparse, AO, etc. should be last to get imported, as nothing\u001b[39;00m\n\u001b[0;32m   2186\u001b[0m \u001b[38;5;66;03m# is expected to depend on them.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\distributions\\__init__.py:79\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbinomial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Binomial\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Categorical\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcauchy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cauchy\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchi2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chi2\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstraint_registry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m biject_to, transform_to\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1528\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1502\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1601\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 필요한 모듈 import\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"\"), \"..\"))\n",
    "import custom\n",
    "import numpy as np\n",
    "\n",
    "# Data 전처리 모듈 : custom과 numpy만으로 가능함\n",
    "from tqdm import tqdm #자료가 많으니 진행상황은 봅시다\n",
    "\n",
    "# AI 만들기 모듈\n",
    "import torch\n",
    "import torch.nn as nn #클래스 형식의 신경망 함수\n",
    "import torch.nn.functional as F #함수 형식의 신경망 함수\n",
    "from torch.utils.data import DataLoader #데이터 batch 나눠주는 역할의 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374ef61-4566-42a8-bfa1-597ddcb85d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 숫자 벡터 사전 만들기\n",
    "## ' ',1,2,3,4,5,6,7,8,9,0,+ : 12개\n",
    "## ' '(pad 문자) 는 0행렬\n",
    "## 나머지 11개는 원 핫 인코딩\n",
    "\n",
    "#벡터 값 만들기\n",
    "vector_list = np.zeros((1,11)) # np.array의 백터 값 저장할 pad값만 저장된 행렬\n",
    "vector_list = np.append(vector_list, np.eye(11), axis = 0) # 대각선 값은 1이고 나머지는 0인 정사각 행렬\n",
    "print(vector_list)\n",
    "print(vector_list.shape)\n",
    "\n",
    "#숫자-라벨링 사전\n",
    "num_dict = {}\n",
    "num_dict[' '] = 0\n",
    "\n",
    "for i in range(1,10) : #1~9까지 반복문으로 넣기\n",
    "    num_dict[str(i)] = i\n",
    "num_dict['0'] = 10\n",
    "num_dict['+'] = 11\n",
    "print(num_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd6a5a-058b-4305-b208-efc3cb2fd7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 불러오기\n",
    "file_name = 'addition.txt'\n",
    "with open(file_name, mode = \"r\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "print(len(data))\n",
    "print(data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244718da-56d7-4bfa-b0f6-6de4d45eb3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 x, t로 나누기\n",
    "data_x = []\n",
    "data_t = []\n",
    "\n",
    "for d in data :\n",
    "    d = d.split(\"_\")\n",
    "    data_x.append(d[0])\n",
    "    data_t.append(d[1].replace('\\n','')) #개행문자 제거\n",
    "\n",
    "print(\"x의 갯수 :\",len(data_x))\n",
    "print(\"t의 갯수 :\",len(data_t))\n",
    "print(\"10번째 x :\",data_x[10])\n",
    "print(\"10번째 t :\",data_t[10])\n",
    "print(\"10번째 x 길이 :\",len(data_x[10]))\n",
    "print(\"10번째 t 길이 :\",len(data_t[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9a3d9-7065-44d2-b943-a7015f121c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 라벨링=정수로 바꾸기(신경망 안에 벡터 사전이 있을 때) or 벡터화=1차원 배열(신경망 안에 벡터 사진이 없을 때)\n",
    "## 라벨링 하겠습니다 (= 신경망 안에 벡터 list 넣겠습니다)\n",
    "\n",
    "for i in range(len(data_x)) :\n",
    "    temp = data_x[i]\n",
    "    temp = list(temp) #문장을 문자 하나하나씩 끊어서 list화 #역산 하려면 \"\".join() 쓰면 됩니다\n",
    "    vector = custom.word_vectorize(temp, num_dict, 7)\n",
    "    data_x[i] = vector\n",
    "data_x = np.array(data_x)\n",
    "\n",
    "for i in range(len(data_t)) :\n",
    "    temp = data_t[i]\n",
    "    temp = list(temp)\n",
    "    vector = custom.word_vectorize(temp, num_dict, 4)\n",
    "    data_t[i] = vector\n",
    "data_t = np.array(data_t)\n",
    "\n",
    "print(data_x.shape)\n",
    "print(data_t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45efbc64-1232-41e0-bac8-f18ae3f160c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AI 학습\n",
    "### DataLoader에 싣기 (x, t 입력)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tensor_x = torch.tensor(data_x, dtype = torch.long, device = device)\n",
    "tensor_t = torch.tensor(data_t, dtype = torch.long, device = device)\n",
    "\n",
    "### 앞의 40000번째 까지는 train, 뒤의 10000개는 test(val)\n",
    "s = 40000\n",
    "train_zip = list(zip(tensor_x[:s], tensor_t[:s])) \n",
    "test_zip = list(zip(tensor_x[s:], tensor_t[s:]))\n",
    "\n",
    "train_dataloader = DataLoader(train_zip, batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(test_zip, batch_size=1000, shuffle=False)\n",
    "print(len(train_dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea636c0e-8cf4-4f5f-9c6f-1515f670b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoder와 Decoder 구현 하기 (함수 만들기)\n",
    "\n",
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, vector) :\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(vector, freeze = True, padding_idx=0) # 벡터화 함수, b값이 없는 선형함수, 차원 = (단어갯수, 벡터갯수)\n",
    "        #.from_pretrained : W값을 해당 벡터로 만들겠다\n",
    "        self.rnn = nn.LSTM(vector.shape[1], vector.shape[1], batch_first = True, bidirectional=True)# RNN 함수\n",
    "    def forward(self, x) :\n",
    "        x = self.embedding(x)\n",
    "        y, hc = self.rnn(x) #LSTM은 hc\n",
    "        return y, hc\n",
    "\n",
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, vector, max_len) :\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(vector, freeze = True, padding_idx=0) #벡터화 함수\n",
    "        self.rnn = nn.LSTM(vector.shape[1], vector.shape[1], batch_first = True) # RNN 함수\n",
    "        self.f = nn.Linear(vector.shape[1], vector.shape[0]) #벡터 값을 다시 단어 라벨값으로 역산하는 함수\n",
    "        self.max_len = max_len #문장 단어의 최대 갯수\n",
    "    def forward(self, encoder_output, encoder_hc, t = None) :\n",
    "        decoder_y_list = [] #출력값\n",
    "\n",
    "        decoder_x = torch.zeros((encoder_output.shape[0],1)).type(torch.long).to(encoder_output.device) #첫번째 입력값, 라벨값이기 때문에 차원 = (문장 갯수,1), 문장 갯수는 encoder_output에서 가져옵니다\n",
    "        #.type(torch.long) 은 형변환, .to(encoder_output.device) 은 기기 배정\n",
    "        encoder_h_for = encoder_hc[0][::2]\n",
    "        encoder_h_back= encoder_hc[0][1::2]\n",
    "        encoder_h = encoder_h_for + encoder_h_back\n",
    "        encoder_c_for = encoder_hc[1][::2]\n",
    "        encoder_c_back= encoder_hc[1][1::2]\n",
    "        encoder_c = encoder_c_for + encoder_c_back\n",
    "        decoder_hc = (encoder_h, encoder_c) #첫번째 hc값      \n",
    "\n",
    "        for i in range(self.max_len) :\n",
    "            decoder_y, decoder_hc = self.forward_cal(decoder_x, decoder_hc) # RNN 계산 한블록 합니다.\n",
    "\n",
    "            if t is not None : #teaching force, t가 존재할 때\n",
    "                decoder_x = t[:,i:i+1]  # i번째 t의 단어 가져옴\n",
    "            else : # greedy, t가 존재하지 않을 때\n",
    "                decoder_x = torch.argmax(decoder_y, dim = -1).detach() #출력 값 중에 가장 큰값(라벨값) 가져오고, 해당 값은 미분 대상이 아님 (.detach())\n",
    "\n",
    "            decoder_y_list.append(decoder_y) #출력값에 단어 하나하나 씩 저장\n",
    "\n",
    "        decoder_y_list = torch.cat(decoder_y_list, dim = 1) #list를 tensor로 묶기 위해서 cat 함수 쓰는 것 decoder_y.shape = (문장 갯수, 단어 갯수(1개), 벡터 갯수)\n",
    "        \n",
    "        return decoder_y_list, decoder_hc, None # decoder layer 2개 이상 묶어서 쓰기 위해서 hc값과, attention값도 return하기 위해 3개의 return 값\n",
    "    def forward_cal(self, x, hc) : #rnn 블록하나 계산하는 함수\n",
    "        x = self.embedding(x) # 벡터로 변환하고\n",
    "        x, hc = self.rnn(x, hc) # rnn 계산, hc값은 이전 hc계산 결과, hc값도 따로 입력합니다\n",
    "        x = self.f(x) # 벡터 계산값 역산해서 다시 라벨로 변환\n",
    "        return x, hc\n",
    "        \n",
    "class Encoder_n_Decoder(nn.Module) : #encoder와 decoder를 하나로 묶는 새로운 신경망\n",
    "    def __init__(self, encoder, decoder) :\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, x, t = None) :\n",
    "        y, hc = self.encoder(x)\n",
    "        y, _, _ = self.decoder(y, hc, t)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b67dfe3-14c5-4759-bbc2-130cc55c3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_file_name = 'data/encoder3.pt'\n",
    "decoder_file_name = 'data/decoder3.pt'\n",
    "\n",
    "tensor_vector = torch.tensor(vector_list, dtype = torch.float)\n",
    "encoder = Encoder(tensor_vector).to(device)\n",
    "decoder = Decoder(tensor_vector, 4).to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "encoder_optim = torch.optim.Adam(encoder.parameters(), lr = 0.02)\n",
    "decoder_optim = torch.optim.Adam(decoder.parameters(), lr = 0.02)\n",
    "\n",
    "epoch = 100\n",
    "prev_acc = 0\n",
    "cnt = 0\n",
    "\n",
    "for e in range(epoch) :\n",
    "    loss_sum = 0\n",
    "    encoder.train() #dropout 켜주기\n",
    "    decoder.train()\n",
    "    for x, t in train_dataloader :\n",
    "### y = F(x) (순전파)\n",
    "        y, hc = encoder(x)\n",
    "        y, _, _ = decoder(y, hc)\n",
    "### 손실함수\n",
    "        loss = loss_func(y.reshape(-1, y.shape[-1]) , t.reshape(-1)) #3차원을 2차원으로 펴주기\n",
    "        loss_sum += loss.item()\n",
    "### 역전파\n",
    "        loss.backward()\n",
    "        decoder_optim.step()\n",
    "        encoder_optim.step()\n",
    "        decoder_optim.zero_grad()\n",
    "        encoder_optim.zero_grad()\n",
    "    loss_sum /= len(train_dataloader) #평균 구하기\n",
    "### 중간 점검\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    encoder.eval() #dropout 꺼주기\n",
    "    decoder.eval()\n",
    "    for x, t in test_dataloader :\n",
    "        with torch.no_grad() :\n",
    "            y, hc = encoder(x)\n",
    "            y, _, _ = decoder(y, hc)\n",
    "            correct += (y.argmax(dim = -1) == t).sum().item()\n",
    "        total += len(x) * 4\n",
    "    acc = correct / total\n",
    "### earlystopper\n",
    "    if acc <= prev_acc :\n",
    "        cnt += 1\n",
    "    else :\n",
    "        cnt = 0\n",
    "        prev_acc = acc\n",
    "        torch.save(encoder, encoder_file_name) #중간 저장\n",
    "        torch.save(decoder, decoder_file_name)\n",
    "    print(f\"epoch {e+1} | loss {loss_sum} | acc {acc} | cnt {cnt}\")\n",
    "    if cnt >= 5 :\n",
    "        print(\"train stopped\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba0a399-07bb-49aa-abf4-48c626877899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx로 변환하기 쉽게 하기 위해 다시 불러와서 cpu로 변환 한 다음에 하나로 묶는 작업\n",
    "\n",
    "encoder = torch.load(encoder_file_name, weights_only=False, map_location='cpu') #cpu로 불러오기 \n",
    "decoder = torch.load(decoder_file_name, weights_only=False, map_location='cpu')\n",
    "\n",
    "F = Encoder_n_Decoder(encoder, decoder)\n",
    "print(F)\n",
    "print(F.state_dict()['encoder.embedding.weight']) #.state_dict() : 신경망 내의 W, b 값 불러오는 함수\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1727c-3dad-4901-ad9f-02f21de551c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 묶은 신경망을 다시 저장\n",
    "\n",
    "torch.save(F, \"data/add_ai3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3f1cce-5e92-46b1-bf09-0d04d6d31ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
