{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dced8f0-93ca-4b94-ab60-1f2b478d8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname(\"\"), \"..\"))\n",
    "import custom\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201a7b3a-a67d-4088-b84e-c969c78ca59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2890\n",
      "2871\n"
     ]
    }
   ],
   "source": [
    "# 단어 불러오기\n",
    "\n",
    "with open(\"data/eng_word.json\", mode = \"r\", encoding=\"UTF8\") as f:\n",
    "    eng_word = json.load(f)\n",
    "\n",
    "with open(\"data/kor_word.json\", mode = \"r\", encoding=\"UTF8\") as f:\n",
    "    kor_word = json.load(f)\n",
    "\n",
    "\n",
    "print(len(eng_word))\n",
    "print(len(kor_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf1247f-8990-48f2-9c1f-fefe7deaf65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>eng_len</th>\n",
       "      <th>kor</th>\n",
       "      <th>kor_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>go .</td>\n",
       "      <td>2</td>\n",
       "      <td>가 어 . &lt;eos&gt;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hi .</td>\n",
       "      <td>2</td>\n",
       "      <td>안녕 . &lt;eos&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run !</td>\n",
       "      <td>2</td>\n",
       "      <td>뛰 어 ! &lt;eos&gt;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run .</td>\n",
       "      <td>2</td>\n",
       "      <td>뛰 어 . &lt;eos&gt;</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who ?</td>\n",
       "      <td>2</td>\n",
       "      <td>누구 ? &lt;eos&gt;</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5438</th>\n",
       "      <td>there are some important questions that scienc...</td>\n",
       "      <td>10</td>\n",
       "      <td>과학 이 답변 하 ㄹ 수 없 는 몇 가지 중요 하 ㄴ 질문 들 이 있 어 . &lt;eos&gt;</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5439</th>\n",
       "      <td>company regulations mandate that workers use p...</td>\n",
       "      <td>9</td>\n",
       "      <td>회사 내규 에 의하 면 근로자 가 보호 안경 을 사용 하 도록 되 어 있 다 . &lt;...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5440</th>\n",
       "      <td>company regulations mandate that workers use p...</td>\n",
       "      <td>9</td>\n",
       "      <td>회사 규칙 에 따르 면 근로자 가 보호 안경 을 쓰 도록 되 어 있 어 . &lt;eos&gt;</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5441</th>\n",
       "      <td>company regulations mandate that workers use p...</td>\n",
       "      <td>9</td>\n",
       "      <td>회사 방침 에 따르 면 근로자 가 보호 안경 을 사용 하 도록 되 어 있 어 요 ....</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5442</th>\n",
       "      <td>switzerland is situated between france italy a...</td>\n",
       "      <td>10</td>\n",
       "      <td>스위스 는 프랑스 와 이탈리아 오스트리아 독일 사이 에 있 다 . &lt;eos&gt;</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5443 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    eng  eng_len  \\\n",
       "0                                                  go .        2   \n",
       "1                                                  hi .        2   \n",
       "2                                                 run !        2   \n",
       "3                                                 run .        2   \n",
       "4                                                 who ?        2   \n",
       "...                                                 ...      ...   \n",
       "5438  there are some important questions that scienc...       10   \n",
       "5439  company regulations mandate that workers use p...        9   \n",
       "5440  company regulations mandate that workers use p...        9   \n",
       "5441  company regulations mandate that workers use p...        9   \n",
       "5442  switzerland is situated between france italy a...       10   \n",
       "\n",
       "                                                    kor  kor_len  \n",
       "0                                           가 어 . <eos>        4  \n",
       "1                                            안녕 . <eos>        3  \n",
       "2                                           뛰 어 ! <eos>        4  \n",
       "3                                           뛰 어 . <eos>        4  \n",
       "4                                            누구 ? <eos>        3  \n",
       "...                                                 ...      ...  \n",
       "5438   과학 이 답변 하 ㄹ 수 없 는 몇 가지 중요 하 ㄴ 질문 들 이 있 어 . <eos>       20  \n",
       "5439  회사 내규 에 의하 면 근로자 가 보호 안경 을 사용 하 도록 되 어 있 다 . <...       19  \n",
       "5440    회사 규칙 에 따르 면 근로자 가 보호 안경 을 쓰 도록 되 어 있 어 . <eos>       18  \n",
       "5441  회사 방침 에 따르 면 근로자 가 보호 안경 을 사용 하 도록 되 어 있 어 요 ....       20  \n",
       "5442         스위스 는 프랑스 와 이탈리아 오스트리아 독일 사이 에 있 다 . <eos>       13  \n",
       "\n",
       "[5443 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "data = pd.read_csv(\"data/data.csv\", encoding = \"UTF8\", sep = \"\\t\")\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "328dd1af-0a75-469c-9e56-1cc36551f37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5443, 20)\n",
      "(5443, 10)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 x, t로 나누고, 벡터화\n",
    "kor_len = data['kor_len'].max()\n",
    "eng_len = data['eng_len'].max()\n",
    "\n",
    "data_x = []\n",
    "for i in range(len(data)) :\n",
    "    words = data.iloc[i,2].split()\n",
    "    label = custom.word_vectorize(words, kor_word, kor_len, padding_front=False)\n",
    "    data_x.append(label)\n",
    "data_x = np.array(data_x)\n",
    "print(data_x.shape)\n",
    "\n",
    "data_t = []\n",
    "for i in range(len(data)) :\n",
    "    words = data.iloc[i,0].split()\n",
    "    label = custom.word_vectorize(words, eng_word, eng_len, padding_front=False)\n",
    "    data_t.append(label)\n",
    "data_t = np.array(data_t)\n",
    "print(data_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e41520-1e85-42f5-b786-51507dac51e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "# DataLoader로 나누기\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tensor_x = torch.tensor(data_x, dtype = torch.long, device = device)\n",
    "tensor_t = torch.tensor(data_t, dtype = torch.long, device = device)\n",
    "\n",
    "dataloader = DataLoader(list(zip(tensor_x, tensor_t)), batch_size=100, shuffle=True)\n",
    "test_dataloader = DataLoader(list(zip(tensor_x, tensor_t)), batch_size=100, shuffle=False)\n",
    "print(len(dataloader))\n",
    "print(len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ddabc92-4a1d-4d70-b614-8e7d04e3a7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "### Encoder와 Decoder 구현 하기 (함수 만들기)\n",
    "\n",
    "class Encoder(nn.Module) :\n",
    "    def __init__(self, word_len, vector_size) :\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(word_len, vector_size, padding_idx=0) # 벡터화 함수, b값이 없는 선형함수, 차원 = (단어갯수, 벡터갯수)\n",
    "        #.from_pretrained : W값을 해당 벡터로 만들겠다\n",
    "        self.rnn = nn.LSTM(vector_size, vector_size, batch_first = True, bidirectional=True)# RNN 함수\n",
    "    def forward(self, x) :\n",
    "        x = self.embedding(x)\n",
    "        y, hc = self.rnn(x) #LSTM은 hc\n",
    "        return y, hc\n",
    "\n",
    "class Attention(nn.Module) : #Value를 구하기 위한 재료,\n",
    "    def __init__(self, h_size, bidirectional = False) : # h_size : 벡터 크기\n",
    "        super().__init__()\n",
    "        factor = 2 if bidirectional else 1\n",
    "        self.U = nn.Linear(h_size * factor, h_size * factor) # query(decoder h값) 가공, query.shape = [b, 1, h_size * 2]\n",
    "        self.W = nn.Linear(h_size * factor, h_size * factor) # key(encoder 출력값) 가공, key.shape = [b, f, h_size * 2]\n",
    "        self.V = nn.Linear(h_size * factor, 1) # score(query & key) 계산\n",
    "    def forward(self, query, key) :\n",
    "        score = self.V(torch.tanh(self.U(query) + self.W(key))) # score.shape = [b, f, 1]\n",
    "        score = score.permute(0,2,1) # score.shape = [b,1,f]\n",
    "\n",
    "        weight = torch.softmax(score, dim = -1) # softmax는 함수 차원을 바꾸지 않습니다\n",
    "        context = torch.bmm(weight, key) # [b,1,f] * [b,f,h_size * 2] = [b,1,h_size * 2]\n",
    "        return context, weight\n",
    "\n",
    "class Decoder(nn.Module) :\n",
    "    def __init__(self, word_len, vector_size, max_len) :\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(word_len, vector_size, padding_idx=0) #벡터화 함수\n",
    "        self.attention = Attention(vector_size, bidirectional = True)\n",
    "        self.rnn = nn.LSTM(vector_size * 3, vector_size, batch_first = True, bidirectional=True) # RNN 함수\n",
    "        self.f = nn.Linear(vector_size * 2, word_len) #벡터 값을 다시 단어 라벨값으로 역산하는 함수\n",
    "        self.max_len = max_len #문장 단어의 최대 갯수\n",
    "    def forward(self, encoder_output, encoder_hc, t = None) :        \n",
    "        decoder_y_list = [] #출력값\n",
    "        weight_list = []\n",
    "\n",
    "        decoder_x = torch.zeros((encoder_output.shape[0],1)).type(torch.long).to(encoder_output.device) #첫번째 입력값, 라벨값이기 때문에 차원 = (문장 갯수,1), 문장 갯수는 encoder_output에서 가져옵니다\n",
    "        #.type(torch.long) 은 형변환, .to(encoder_output.device) 은 기기 배정\n",
    "        decoder_hc = encoder_hc #첫번째 hc값\n",
    "\n",
    "        for i in range(self.max_len) :\n",
    "            decoder_y, decoder_hc, weight = self.forward_cal(decoder_x, decoder_hc, encoder_output) # RNN 계산 한블록 합니다.\n",
    "\n",
    "            if t is not None : #teaching force, t가 존재할 때\n",
    "                decoder_x = t[:,i:i+1]  # i번째 t의 단어 가져옴\n",
    "            else : # greedy, t가 존재하지 않을 때\n",
    "                decoder_x = torch.argmax(decoder_y, dim = -1).detach() #출력 값 중에 가장 큰값(라벨값) 가져오고, 해당 값은 미분 대상이 아님 (.detach())\n",
    "\n",
    "            decoder_y_list.append(decoder_y) #출력값에 단어 하나하나 씩 저장\n",
    "            weight_list.append(weight)\n",
    "\n",
    "        decoder_y_list = torch.cat(decoder_y_list, dim = 1) #list를 tensor로 묶기 위해서 cat 함수 쓰는 것 decoder_y.shape = (문장 갯수, 단어 갯수(1개), 벡터 갯수)\n",
    "        weight_list = torch.cat(weight_list, dim = 1)\n",
    "        \n",
    "        return decoder_y_list, decoder_hc, weight_list # decoder layer 2개 이상 묶어서 쓰기 위해서 hc값과, attention값도 return하기 위해 3개의 return 값\n",
    "    def forward_cal(self, x, hc, encoder_output) : #rnn 블록하나 계산하는 함수\n",
    "        h_for = hc[0][::2]\n",
    "        h_back = hc[0][1::2]\n",
    "        query = torch.cat([h_for, h_back], dim = -1).permute(1,0,2)\n",
    "        \n",
    "        context, weight = self.attention(query, encoder_output)\n",
    "        \n",
    "        x = self.embedding(x) # 벡터로 변환하고\n",
    "        x = torch.cat([context, x], dim = -1) # attention 적용    \n",
    "        x, hc = self.rnn(x, hc) # rnn 계산, hc값은 이전 hc계산 결과, hc값도 따로 입력합니다\n",
    "        x = self.f(x) # 벡터 계산값 역산해서 다시 라벨로 변환\n",
    "        return x, hc, weight\n",
    "        \n",
    "class Encoder_n_Decoder(nn.Module) : #encoder와 decoder를 하나로 묶는 새로운 신경망\n",
    "    def __init__(self, encoder, decoder) :\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self, x, t = None) :\n",
    "        y, hc = self.encoder(x)\n",
    "        y, _, _ = self.decoder(y, hc, t)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a93bf15-869c-441d-9aef-845a06712945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 | loss 4.632988747683439 | acc 0.2544650751547303 | cnt 0\n",
      "epoch 2 | loss 3.049969226663763 | acc 0.3032419687592101 | cnt 0\n",
      "epoch 3 | loss 2.0987522797151046 | acc 0.3893309755378721 | cnt 0\n",
      "epoch 4 | loss 1.4614329034631903 | acc 0.5264367816091954 | cnt 0\n",
      "epoch 5 | loss 1.0429541035131975 | acc 0.6206896551724138 | cnt 0\n",
      "epoch 6 | loss 0.742204714905132 | acc 0.7109048040082523 | cnt 0\n",
      "epoch 7 | loss 0.5537535537372936 | acc 0.7720601237842617 | cnt 0\n",
      "epoch 8 | loss 0.4128740971738642 | acc 0.8255820807544946 | cnt 0\n",
      "epoch 9 | loss 0.3232164900411259 | acc 0.8704980842911877 | cnt 0\n",
      "epoch 10 | loss 0.256750061024319 | acc 0.8871205422929561 | cnt 0\n",
      "epoch 11 | loss 0.21348540417172693 | acc 0.9171529619805482 | cnt 0\n",
      "epoch 12 | loss 0.18524594686248086 | acc 0.9228706159740643 | cnt 0\n",
      "epoch 13 | loss 0.16675427095456558 | acc 0.9345711759504863 | cnt 0\n",
      "epoch 14 | loss 0.14746667566624555 | acc 0.9407898614795166 | cnt 0\n",
      "epoch 15 | loss 0.13898776783184572 | acc 0.9445328617742411 | cnt 0\n",
      "epoch 16 | loss 0.1297445690090006 | acc 0.9480106100795755 | cnt 0\n",
      "epoch 17 | loss 0.13789256878874517 | acc 0.9322428529325081 | cnt 1\n",
      "epoch 18 | loss 0.17118948589671742 | acc 0.8985853227232538 | cnt 2\n",
      "epoch 19 | loss 0.244257182831114 | acc 0.8089301503094607 | cnt 3\n",
      "epoch 20 | loss 0.49522212039340624 | acc 0.6445918066607722 | cnt 4\n",
      "epoch 21 | loss 0.9008371867916801 | acc 0.5330975537872089 | cnt 5\n",
      "train stopped\n"
     ]
    }
   ],
   "source": [
    "encoder_file_name = 'data/encoder.pt'\n",
    "decoder_file_name = 'data/decoder.pt'\n",
    "pad_idx = 0\n",
    "\n",
    "encoder = Encoder(len(kor_word), 100).to(device)\n",
    "decoder = Decoder(len(eng_word), 100, 10).to(device)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "encoder_optim = torch.optim.Adam(encoder.parameters(), lr = 0.02)\n",
    "decoder_optim = torch.optim.Adam(decoder.parameters(), lr = 0.02)\n",
    "\n",
    "epoch = 100\n",
    "prev_acc = 0\n",
    "cnt = 0\n",
    "\n",
    "for e in range(epoch) :\n",
    "    loss_sum = 0\n",
    "    encoder.train() #dropout 켜주기\n",
    "    decoder.train()\n",
    "    for x, t in dataloader :\n",
    "### y = F(x) (순전파)\n",
    "        y, hc = encoder(x)\n",
    "        y, _, _ = decoder(y, hc, t)\n",
    "### 손실함수\n",
    "        loss = loss_func(y.reshape(-1, y.shape[-1]) , t.reshape(-1)) #3차원을 2차원으로 펴주기\n",
    "        loss_sum += loss.item()\n",
    "### 역전파\n",
    "        loss.backward()\n",
    "        decoder_optim.step()\n",
    "        encoder_optim.step()\n",
    "        decoder_optim.zero_grad()\n",
    "        encoder_optim.zero_grad()\n",
    "    loss_sum /= len(dataloader) #평균 구하기\n",
    "### 중간 점검\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    encoder.eval() #dropout 꺼주기\n",
    "    decoder.eval()\n",
    "    for x, t in test_dataloader :\n",
    "        with torch.no_grad() :\n",
    "            t_padding_mask = (t != pad_idx)\n",
    "            y, hc = encoder(x)\n",
    "            y, _, _ = decoder(y, hc)\n",
    "            correct += (y.argmax(dim = -1)[t_padding_mask] == t[t_padding_mask]).sum().item()\n",
    "        total += t_padding_mask.sum().item()\n",
    "    acc = correct / total\n",
    "### earlystopper\n",
    "    if acc <= prev_acc :\n",
    "        cnt += 1\n",
    "    else :\n",
    "        cnt = 0\n",
    "        prev_acc = acc\n",
    "        torch.save(encoder, encoder_file_name) #중간 저장\n",
    "        torch.save(decoder, decoder_file_name)\n",
    "    print(f\"epoch {e+1} | loss {loss_sum} | acc {acc} | cnt {cnt}\")\n",
    "    if cnt >= 5 :\n",
    "        print(\"train stopped\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59d5a5-f763-4105-b1d0-26049a96d195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
