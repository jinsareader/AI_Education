{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642b1360-657a-42bf-99d2-475e84004987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onnx로 바꾸기\n",
    "\n",
    "# 신경망 함수 불러오기 (torch)\n",
    "# onnx로 변환하기 (torch.onnx)\n",
    "# 잘 저장 되었는지 확인하기 (onnx, onnxrunetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da7df497-9068-4fdb-9b87-dd18cba51c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module import\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.onnx\n",
    "import onnx\n",
    "import onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6393863d-669b-404f-ada1-5f9e33ea1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 함수 NN.py import\n",
    "\n",
    "from NN import PositionEncoding\n",
    "from NN import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9f197e-c60f-48a4-bf69-07352cb1459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (embedding): Embedding(30187, 200, padding_idx=0)\n",
      "  (pos_encoder): PositionEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=200, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (linear2): Linear(in_features=1024, out_features=200, bias=True)\n",
      "    (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.1, inplace=False)\n",
      "    (dropout2): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=200, out_features=200, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=200, out_features=1024, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=1024, out_features=200, bias=True)\n",
      "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (f): Sequential(\n",
      "    (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 신경망 load\n",
    "\n",
    "encoder = torch.load(\"encoder.pt\", weights_only=False, map_location=\"cpu\")\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fa67cf3-d154-49fc-b31d-46c81a5dd132",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_784\\2608939146.py:6: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `Encoder([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `Encoder([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 20 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 20},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.7.0+cu126',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"x\"<INT64,[1,s0]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"y\"<FLOAT,[1,7]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"embedding.weight\"<FLOAT,[30187,200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.self_attn.in_proj_weight\"<FLOAT,[600,200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.self_attn.in_proj_bias\"<FLOAT,[600]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.self_attn.out_proj.weight\"<FLOAT,[200,200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.self_attn.out_proj.bias\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.linear1.weight\"<FLOAT,[1024,200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.linear1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.linear2.weight\"<FLOAT,[200,1024]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.linear2.bias\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.norm1.weight\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.norm1.bias\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.norm2.weight\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.0.norm2.bias\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.self_attn.in_proj_weight\"<FLOAT,[600,200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.self_attn.in_proj_bias\"<FLOAT,[600]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.self_attn.out_proj.weight\"<FLOAT,[200,200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.self_attn.out_proj.bias\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.linear1.weight\"<FLOAT,[1024,200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.linear1.bias\"<FLOAT,[1024]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.linear2.weight\"<FLOAT,[200,1024]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.linear2.bias\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.norm1.weight\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.norm1.bias\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.norm2.weight\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"encoder.layers.1.norm2.bias\"<FLOAT,[200]>{TorchTensor(...)},\n",
       "                %\"f.0.weight\"<FLOAT,[100,200]>{TorchTensor(...)},\n",
       "                %\"f.0.bias\"<FLOAT,[100]>{TorchTensor(...)},\n",
       "                %\"f.2.weight\"<FLOAT,[7,100]>{TorchTensor(...)},\n",
       "                %\"f.2.bias\"<FLOAT,[7]>{TorchTensor<FLOAT,[7]>(Parameter containing: tensor([-0.0399,  0.1111, -0.1004,  0.0032,  0.0051, -0.1628, -0.1448], requires_grad=True), name='f.2.bias')}\n",
       "            ),\n",
       "        ) {\n",
       "              0 |  # node_Shape_0\n",
       "                   %\"val_0\"<INT64,[1]> ⬅️ ::Shape(%\"x\") {start=1, end=2}\n",
       "              1 |  # node_Gather_2\n",
       "                   %\"embedding\"<FLOAT,[1,s0,200]> ⬅️ ::Gather(%\"embedding.weight\"{...}, %\"x\") {axis=0}\n",
       "              2 |  # node_Constant_3\n",
       "                   %\"val_1\"<FLOAT,[]>{Tensor<FLOAT,[]>(array(14.142136, dtype=float32), name=None)} ⬅️ ::Constant() {value=Tensor<FLOAT,[]>(array(14.142136, dtype=float32), name=None)}\n",
       "              3 |  # node_Mul_4\n",
       "                   %\"mul_3\"<FLOAT,[1,s0,200]> ⬅️ ::Mul(%\"embedding\", %\"val_1\"{14.142135620117188})\n",
       "              4 |  # node_Constant_5\n",
       "                   %\"val_2\"<INT64,[]>{Tensor<INT64,[]>(array(0, dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(0, dtype=int64), name=None)}\n",
       "              5 |  # node_Constant_7\n",
       "                   %\"val_4\"<INT64,[]>{Tensor<INT64,[]>(array(2, dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(2, dtype=int64), name=None)}\n",
       "              6 |  # node_Transpose_10\n",
       "                   %\"transpose\"<FLOAT,[s0,1,200]> ⬅️ ::Transpose(%\"mul_3\") {perm=[1, 0, 2]}\n",
       "              7 |  # node_Transpose_11\n",
       "                   %\"val_5\"<FLOAT,[200,600]> ⬅️ ::Transpose(%\"encoder.layers.0.self_attn.in_proj_weight\"{...}) {perm=[1, 0]}\n",
       "              8 |  # node_MatMul_12\n",
       "                   %\"val_6\"<FLOAT,[s0,1,600]> ⬅️ ::MatMul(%\"transpose\", %\"val_5\")\n",
       "              9 |  # node_Add_13\n",
       "                   %\"linear\"<FLOAT,[s0,1,600]> ⬅️ ::Add(%\"val_6\", %\"encoder.layers.0.self_attn.in_proj_bias\"{...})\n",
       "             10 |  # node_Constant_16\n",
       "                   %\"val_9\"<INT64,[1]>{Tensor<INT64,[1]>(array([1], dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([1], dtype=int64), name=None)}\n",
       "             11 |  # node_Constant_17\n",
       "                   %\"val_10\"<INT64,[1]>{Tensor<INT64,[1]>(array([3], dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([3], dtype=int64), name=None)}\n",
       "             12 |  # node_Constant_18\n",
       "                   %\"val_11\"<INT64,[1]>{Tensor<INT64,[1]>(array([200], dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([200], dtype=int64), name=None)}\n",
       "             13 |  # node_Concat_19\n",
       "                   %\"val_12\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_9\"{[1]}, %\"val_10\"{[3]}, %\"val_11\"{[200]}) {axis=0}\n",
       "             14 |  # node_Reshape_21\n",
       "                   %\"view\"<FLOAT,[s0,1,3,200]> ⬅️ ::Reshape(%\"linear\", %\"val_12\") {allowzero=True}\n",
       "             15 |  # node_Constant_22\n",
       "                   %\"val_14\"<INT64,[1]>{Tensor<INT64,[1]>(array([0], dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([0], dtype=int64), name=None)}\n",
       "             16 |  # node_Unsqueeze_23\n",
       "                   %\"unsqueeze_3\"<FLOAT,[1,s0,1,3,200]> ⬅️ ::Unsqueeze(%\"view\", %\"val_14\"{[0]})\n",
       "             17 |  # node_Transpose_24\n",
       "                   %\"transpose_1\"<FLOAT,[3,s0,1,1,200]> ⬅️ ::Transpose(%\"unsqueeze_3\") {perm=[3, 1, 2, 0, 4]}\n",
       "             18 |  # node_Constant_25\n",
       "                   %\"val_15\"<INT64,[1]>{Tensor<INT64,[1]>(array([-2], dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([-2], dtype=int64), name=None)}\n",
       "             19 |  # node_Squeeze_26\n",
       "                   %\"squeeze\"<FLOAT,[3,s0,1,200]> ⬅️ ::Squeeze(%\"transpose_1\", %\"val_15\"{[-2]})\n",
       "             20 |  # node_Gather_28\n",
       "                   %\"select\"<FLOAT,[s0,1,200]> ⬅️ ::Gather(%\"squeeze\", %\"val_2\"{0}) {axis=0}\n",
       "             21 |  # node_Constant_29\n",
       "                   %\"val_16\"<INT64,[]>{Tensor<INT64,[]>(array(1, dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(1, dtype=int64), name=None)}\n",
       "             22 |  # node_Gather_30\n",
       "                   %\"select_1\"<FLOAT,[s0,1,200]> ⬅️ ::Gather(%\"squeeze\", %\"val_16\"{1}) {axis=0}\n",
       "             23 |  # node_Gather_31\n",
       "                   %\"select_2\"<FLOAT,[s0,1,200]> ⬅️ ::Gather(%\"squeeze\", %\"val_4\"{2}) {axis=0}\n",
       "             24 |  # node_Constant_34\n",
       "                   %\"val_19\"<INT64,[1]>{Tensor<INT64,[1]>(array([8], dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([8], dtype=int64), name=None)}\n",
       "             25 |  # node_Constant_35\n",
       "                   %\"val_20\"<INT64,[1]>{Tensor<INT64,[1]>(array([25], dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([25], dtype=int64), name=None)}\n",
       "             26 |  # node_Concat_36\n",
       "                   %\"val_21\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_19\"{[8]}, %\"val_20\"{[25]}) {axis=0}\n",
       "             27 |  # node_Reshape_38\n",
       "                   %\"view_1\"<FLOAT,[s0,8,25]> ⬅️ ::Reshape(%\"select\", %\"val_21\") {allowzero=True}\n",
       "             28 |  # node_Transpose_39\n",
       "                   %\"transpose_2\"<FLOAT,[8,s0,25]> ⬅️ ::Transpose(%\"view_1\") {perm=[1, 0, 2]}\n",
       "             29 |  # node_Concat_42\n",
       "                   %\"val_25\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_19\"{[8]}, %\"val_20\"{[25]}) {axis=0}\n",
       "             30 |  # node_Reshape_44\n",
       "                   %\"view_2\"<FLOAT,[s0,8,25]> ⬅️ ::Reshape(%\"select_1\", %\"val_25\") {allowzero=True}\n",
       "             31 |  # node_Transpose_45\n",
       "                   %\"transpose_3\"<FLOAT,[8,s0,25]> ⬅️ ::Transpose(%\"view_2\") {perm=[1, 0, 2]}\n",
       "             32 |  # node_Concat_48\n",
       "                   %\"val_29\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_19\"{[8]}, %\"val_20\"{[25]}) {axis=0}\n",
       "             33 |  # node_Reshape_50\n",
       "                   %\"view_3\"<FLOAT,[s0,8,25]> ⬅️ ::Reshape(%\"select_2\", %\"val_29\") {allowzero=True}\n",
       "             34 |  # node_Transpose_51\n",
       "                   %\"transpose_4\"<FLOAT,[8,s0,25]> ⬅️ ::Transpose(%\"view_3\") {perm=[1, 0, 2]}\n",
       "             35 |  # node_Concat_54\n",
       "                   %\"val_33\"<INT64,[4]> ⬅️ ::Concat(%\"val_9\"{[1]}, %\"val_19\"{[8]}, %\"val_0\", %\"val_20\"{[25]}) {axis=0}\n",
       "             36 |  # node_Reshape_56\n",
       "                   %\"view_4\"<FLOAT,[1,8,s0,25]> ⬅️ ::Reshape(%\"transpose_2\", %\"val_33\") {allowzero=True}\n",
       "             37 |  # node_Concat_59\n",
       "                   %\"val_37\"<INT64,[4]> ⬅️ ::Concat(%\"val_9\"{[1]}, %\"val_19\"{[8]}, %\"val_0\", %\"val_20\"{[25]}) {axis=0}\n",
       "             38 |  # node_Reshape_61\n",
       "                   %\"view_5\"<FLOAT,[1,8,s0,25]> ⬅️ ::Reshape(%\"transpose_3\", %\"val_37\") {allowzero=True}\n",
       "             39 |  # node_Concat_64\n",
       "                   %\"val_41\"<INT64,[4]> ⬅️ ::Concat(%\"val_9\"{[1]}, %\"val_19\"{[8]}, %\"val_0\", %\"val_20\"{[25]}) {axis=0}\n",
       "             40 |  # node_Reshape_66\n",
       "                   %\"view_6\"<FLOAT,[1,8,s0,25]> ⬅️ ::Reshape(%\"transpose_4\", %\"val_41\") {allowzero=True}\n",
       "             41 |  # node_Shape_76\n",
       "                   %\"val_52\"<INT64,[4]> ⬅️ ::Shape(%\"view_5\") {start=0}\n",
       "             42 |  # node_Constant_77\n",
       "                   %\"val_53\"<INT64,[1]>{Tensor<INT64,[1]>(array([9223372036854775807], dtype=int64), name='val_53')} ⬅️ ::Constant() {value_ints=[9223372036854775807]}\n",
       "             43 |  # node_Constant_78\n",
       "                   %\"val_54\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1], dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[1]>(array([-1], dtype=int64), name=None)}\n",
       "             44 |  # node_Slice_79\n",
       "                   %\"val_55\"<INT64,[1]> ⬅️ ::Slice(%\"val_52\", %\"val_54\"{[-1]}, %\"val_53\"{[9223372036854775807]})\n",
       "             45 |  # node_Slice_80\n",
       "                   %\"val_56\"<INT64,[1]> ⬅️ ::Slice(%\"val_52\", %\"val_15\"{[-2]}, %\"val_54\"{[-1]})\n",
       "             46 |  # node_Constant_81\n",
       "                   %\"val_57\"<INT64,[1]>{Tensor<INT64,[1]>(array([-9223372036854775808], dtype=int64), name='val_57')} ⬅️ ::Constant() {value_ints=[-9223372036854775808]}\n",
       "             47 |  # node_Slice_82\n",
       "                   %\"val_58\"<INT64,[2]> ⬅️ ::Slice(%\"val_52\", %\"val_57\"{[-9223372036854775808]}, %\"val_15\"{[-2]})\n",
       "             48 |  # node_Constant_83\n",
       "                   %\"val_59\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1], dtype=int64), name='val_59')} ⬅️ ::Constant() {value_ints=[-1]}\n",
       "             49 |  # node_Concat_84\n",
       "                   %\"val_60\"<INT64,[3]> ⬅️ ::Concat(%\"val_59\"{[-1]}, %\"val_56\", %\"val_55\") {axis=0}\n",
       "             50 |  # node_Reshape_85\n",
       "                   %\"val_61\"<FLOAT,[None,None,None]> ⬅️ ::Reshape(%\"view_5\", %\"val_60\") {allowzero=0}\n",
       "             51 |  # node_Transpose_86\n",
       "                   %\"val_62\"<FLOAT,[None,None,None]> ⬅️ ::Transpose(%\"val_61\") {perm=[0, 2, 1]}\n",
       "             52 |  # node_Concat_87\n",
       "                   %\"val_63\"<INT64,[4]> ⬅️ ::Concat(%\"val_58\", %\"val_55\", %\"val_56\") {axis=0}\n",
       "             53 |  # node_Reshape_88\n",
       "                   %\"val_64\"<FLOAT,[None,None,None,None]> ⬅️ ::Reshape(%\"val_62\", %\"val_63\") {allowzero=0}\n",
       "             54 |  # node_Constant_276\n",
       "                   %\"val_65\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.4472136], dtype=float32), name='val_65')} ⬅️ ::Constant() {value=Tensor<FLOAT,[1]>(array([0.4472136], dtype=float32), name='val_65')}\n",
       "             55 |  # node_Mul_90\n",
       "                   %\"val_66\"<FLOAT,[1,8,s0,25]> ⬅️ ::Mul(%\"view_4\", %\"val_65\"{[0.4472135901451111]})\n",
       "             56 |  # node_Constant_279\n",
       "                   %\"val_68\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.4472136], dtype=float32), name='val_68')} ⬅️ ::Constant() {value=Tensor<FLOAT,[1]>(array([0.4472136], dtype=float32), name='val_68')}\n",
       "             57 |  # node_Mul_93\n",
       "                   %\"val_69\"<FLOAT,[None,None,None,None]> ⬅️ ::Mul(%\"val_64\", %\"val_68\"{[0.4472135901451111]})\n",
       "             58 |  # node_MatMul_94\n",
       "                   %\"val_70\"<FLOAT,[None,8,s0,None]> ⬅️ ::MatMul(%\"val_66\", %\"val_69\")\n",
       "             59 |  # node_Softmax_95\n",
       "                   %\"val_71\"<FLOAT,[None,8,s0,None]> ⬅️ ::Softmax(%\"val_70\") {axis=-1}\n",
       "             60 |  # node_MatMul_98\n",
       "                   %\"scaled_dot_product_attention\"<FLOAT,[1,8,s0,25]> ⬅️ ::MatMul(%\"val_71\", %\"view_6\")\n",
       "             61 |  # node_Transpose_99\n",
       "                   %\"permute\"<FLOAT,[s0,1,8,25]> ⬅️ ::Transpose(%\"scaled_dot_product_attention\") {perm=[2, 0, 1, 3]}\n",
       "             62 |  # node_Concat_102\n",
       "                   %\"val_77\"<INT64,[2]> ⬅️ ::Concat(%\"val_0\", %\"val_11\"{[200]}) {axis=0}\n",
       "             63 |  # node_Reshape_104\n",
       "                   %\"view_7\"<FLOAT,[s0,200]> ⬅️ ::Reshape(%\"permute\", %\"val_77\") {allowzero=True}\n",
       "             64 |  # node_Gemm_105\n",
       "                   %\"linear_1\"<FLOAT,[s0,200]> ⬅️ ::Gemm(%\"view_7\", %\"encoder.layers.0.self_attn.out_proj.weight\"{...}, %\"encoder.layers.0.self_attn.out_proj.bias\"{...}) {transA=0, transB=True, alpha=1.0, beta=1.0}\n",
       "             65 |  # node_Concat_108\n",
       "                   %\"val_81\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_9\"{[1]}, %\"val_11\"{[200]}) {axis=0}\n",
       "             66 |  # node_Reshape_110\n",
       "                   %\"view_8\"<FLOAT,[s0,1,200]> ⬅️ ::Reshape(%\"linear_1\", %\"val_81\") {allowzero=True}\n",
       "             67 |  # node_Transpose_111\n",
       "                   %\"transpose_5\"<FLOAT,[1,s0,200]> ⬅️ ::Transpose(%\"view_8\") {perm=[1, 0, 2]}\n",
       "             68 |  # node_Add_113\n",
       "                   %\"add_173\"<FLOAT,[1,s0,200]> ⬅️ ::Add(%\"mul_3\", %\"transpose_5\")\n",
       "             69 |  # node_LayerNormalization_114\n",
       "                   %\"layer_norm\"<FLOAT,[1,s0,200]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_173\", %\"encoder.layers.0.norm1.weight\"{...}, %\"encoder.layers.0.norm1.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "             70 |  # node_Transpose_115\n",
       "                   %\"val_85\"<FLOAT,[200,1024]> ⬅️ ::Transpose(%\"encoder.layers.0.linear1.weight\"{...}) {perm=[1, 0]}\n",
       "             71 |  # node_MatMul_116\n",
       "                   %\"val_86\"<FLOAT,[1,s0,1024]> ⬅️ ::MatMul(%\"layer_norm\", %\"val_85\")\n",
       "             72 |  # node_Add_117\n",
       "                   %\"linear_2\"<FLOAT,[1,s0,1024]> ⬅️ ::Add(%\"val_86\", %\"encoder.layers.0.linear1.bias\"{...})\n",
       "             73 |  # node_Relu_118\n",
       "                   %\"relu\"<FLOAT,[1,s0,1024]> ⬅️ ::Relu(%\"linear_2\")\n",
       "             74 |  # node_Transpose_120\n",
       "                   %\"val_87\"<FLOAT,[1024,200]> ⬅️ ::Transpose(%\"encoder.layers.0.linear2.weight\"{...}) {perm=[1, 0]}\n",
       "             75 |  # node_MatMul_121\n",
       "                   %\"val_88\"<FLOAT,[1,s0,200]> ⬅️ ::MatMul(%\"relu\", %\"val_87\")\n",
       "             76 |  # node_Add_122\n",
       "                   %\"linear_3\"<FLOAT,[1,s0,200]> ⬅️ ::Add(%\"val_88\", %\"encoder.layers.0.linear2.bias\"{...})\n",
       "             77 |  # node_Add_124\n",
       "                   %\"add_195\"<FLOAT,[1,s0,200]> ⬅️ ::Add(%\"layer_norm\", %\"linear_3\")\n",
       "             78 |  # node_LayerNormalization_125\n",
       "                   %\"layer_norm_1\"<FLOAT,[1,s0,200]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_195\", %\"encoder.layers.0.norm2.weight\"{...}, %\"encoder.layers.0.norm2.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "             79 |  # node_Transpose_126\n",
       "                   %\"transpose_6\"<FLOAT,[s0,1,200]> ⬅️ ::Transpose(%\"layer_norm_1\") {perm=[1, 0, 2]}\n",
       "             80 |  # node_Transpose_127\n",
       "                   %\"val_91\"<FLOAT,[200,600]> ⬅️ ::Transpose(%\"encoder.layers.1.self_attn.in_proj_weight\"{...}) {perm=[1, 0]}\n",
       "             81 |  # node_MatMul_128\n",
       "                   %\"val_92\"<FLOAT,[s0,1,600]> ⬅️ ::MatMul(%\"transpose_6\", %\"val_91\")\n",
       "             82 |  # node_Add_129\n",
       "                   %\"linear_4\"<FLOAT,[s0,1,600]> ⬅️ ::Add(%\"val_92\", %\"encoder.layers.1.self_attn.in_proj_bias\"{...})\n",
       "             83 |  # node_Concat_132\n",
       "                   %\"val_95\"<INT64,[4]> ⬅️ ::Concat(%\"val_0\", %\"val_9\"{[1]}, %\"val_10\"{[3]}, %\"val_11\"{[200]}) {axis=0}\n",
       "             84 |  # node_Reshape_134\n",
       "                   %\"view_9\"<FLOAT,[s0,1,3,200]> ⬅️ ::Reshape(%\"linear_4\", %\"val_95\") {allowzero=True}\n",
       "             85 |  # node_Unsqueeze_135\n",
       "                   %\"unsqueeze_4\"<FLOAT,[1,s0,1,3,200]> ⬅️ ::Unsqueeze(%\"view_9\", %\"val_14\"{[0]})\n",
       "             86 |  # node_Transpose_136\n",
       "                   %\"transpose_7\"<FLOAT,[3,s0,1,1,200]> ⬅️ ::Transpose(%\"unsqueeze_4\") {perm=[3, 1, 2, 0, 4]}\n",
       "             87 |  # node_Squeeze_137\n",
       "                   %\"squeeze_1\"<FLOAT,[3,s0,1,200]> ⬅️ ::Squeeze(%\"transpose_7\", %\"val_15\"{[-2]})\n",
       "             88 |  # node_Gather_139\n",
       "                   %\"select_3\"<FLOAT,[s0,1,200]> ⬅️ ::Gather(%\"squeeze_1\", %\"val_2\"{0}) {axis=0}\n",
       "             89 |  # node_Gather_140\n",
       "                   %\"select_4\"<FLOAT,[s0,1,200]> ⬅️ ::Gather(%\"squeeze_1\", %\"val_16\"{1}) {axis=0}\n",
       "             90 |  # node_Gather_141\n",
       "                   %\"select_5\"<FLOAT,[s0,1,200]> ⬅️ ::Gather(%\"squeeze_1\", %\"val_4\"{2}) {axis=0}\n",
       "             91 |  # node_Concat_144\n",
       "                   %\"val_99\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_19\"{[8]}, %\"val_20\"{[25]}) {axis=0}\n",
       "             92 |  # node_Reshape_146\n",
       "                   %\"view_10\"<FLOAT,[s0,8,25]> ⬅️ ::Reshape(%\"select_3\", %\"val_99\") {allowzero=True}\n",
       "             93 |  # node_Transpose_147\n",
       "                   %\"transpose_8\"<FLOAT,[8,s0,25]> ⬅️ ::Transpose(%\"view_10\") {perm=[1, 0, 2]}\n",
       "             94 |  # node_Concat_150\n",
       "                   %\"val_103\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_19\"{[8]}, %\"val_20\"{[25]}) {axis=0}\n",
       "             95 |  # node_Reshape_152\n",
       "                   %\"view_11\"<FLOAT,[s0,8,25]> ⬅️ ::Reshape(%\"select_4\", %\"val_103\") {allowzero=True}\n",
       "             96 |  # node_Transpose_153\n",
       "                   %\"transpose_9\"<FLOAT,[8,s0,25]> ⬅️ ::Transpose(%\"view_11\") {perm=[1, 0, 2]}\n",
       "             97 |  # node_Concat_156\n",
       "                   %\"val_107\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_19\"{[8]}, %\"val_20\"{[25]}) {axis=0}\n",
       "             98 |  # node_Reshape_158\n",
       "                   %\"view_12\"<FLOAT,[s0,8,25]> ⬅️ ::Reshape(%\"select_5\", %\"val_107\") {allowzero=True}\n",
       "             99 |  # node_Transpose_159\n",
       "                   %\"transpose_10\"<FLOAT,[8,s0,25]> ⬅️ ::Transpose(%\"view_12\") {perm=[1, 0, 2]}\n",
       "            100 |  # node_Concat_162\n",
       "                   %\"val_111\"<INT64,[4]> ⬅️ ::Concat(%\"val_9\"{[1]}, %\"val_19\"{[8]}, %\"val_0\", %\"val_20\"{[25]}) {axis=0}\n",
       "            101 |  # node_Reshape_164\n",
       "                   %\"view_13\"<FLOAT,[1,8,s0,25]> ⬅️ ::Reshape(%\"transpose_8\", %\"val_111\") {allowzero=True}\n",
       "            102 |  # node_Concat_167\n",
       "                   %\"val_115\"<INT64,[4]> ⬅️ ::Concat(%\"val_9\"{[1]}, %\"val_19\"{[8]}, %\"val_0\", %\"val_20\"{[25]}) {axis=0}\n",
       "            103 |  # node_Reshape_169\n",
       "                   %\"view_14\"<FLOAT,[1,8,s0,25]> ⬅️ ::Reshape(%\"transpose_9\", %\"val_115\") {allowzero=True}\n",
       "            104 |  # node_Concat_172\n",
       "                   %\"val_119\"<INT64,[4]> ⬅️ ::Concat(%\"val_9\"{[1]}, %\"val_19\"{[8]}, %\"val_0\", %\"val_20\"{[25]}) {axis=0}\n",
       "            105 |  # node_Reshape_174\n",
       "                   %\"view_15\"<FLOAT,[1,8,s0,25]> ⬅️ ::Reshape(%\"transpose_10\", %\"val_119\") {allowzero=True}\n",
       "            106 |  # node_Shape_184\n",
       "                   %\"val_130\"<INT64,[4]> ⬅️ ::Shape(%\"view_14\") {start=0}\n",
       "            107 |  # node_Constant_185\n",
       "                   %\"val_131\"<INT64,[1]>{Tensor<INT64,[1]>(array([9223372036854775807], dtype=int64), name='val_131')} ⬅️ ::Constant() {value_ints=[9223372036854775807]}\n",
       "            108 |  # node_Slice_186\n",
       "                   %\"val_132\"<INT64,[1]> ⬅️ ::Slice(%\"val_130\", %\"val_54\"{[-1]}, %\"val_131\"{[9223372036854775807]})\n",
       "            109 |  # node_Slice_187\n",
       "                   %\"val_133\"<INT64,[1]> ⬅️ ::Slice(%\"val_130\", %\"val_15\"{[-2]}, %\"val_54\"{[-1]})\n",
       "            110 |  # node_Constant_188\n",
       "                   %\"val_134\"<INT64,[1]>{Tensor<INT64,[1]>(array([-9223372036854775808], dtype=int64), name='val_134')} ⬅️ ::Constant() {value_ints=[-9223372036854775808]}\n",
       "            111 |  # node_Slice_189\n",
       "                   %\"val_135\"<INT64,[2]> ⬅️ ::Slice(%\"val_130\", %\"val_134\"{[-9223372036854775808]}, %\"val_15\"{[-2]})\n",
       "            112 |  # node_Constant_190\n",
       "                   %\"val_136\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1], dtype=int64), name='val_136')} ⬅️ ::Constant() {value_ints=[-1]}\n",
       "            113 |  # node_Concat_191\n",
       "                   %\"val_137\"<INT64,[3]> ⬅️ ::Concat(%\"val_136\"{[-1]}, %\"val_133\", %\"val_132\") {axis=0}\n",
       "            114 |  # node_Reshape_192\n",
       "                   %\"val_138\"<FLOAT,[None,None,None]> ⬅️ ::Reshape(%\"view_14\", %\"val_137\") {allowzero=0}\n",
       "            115 |  # node_Transpose_193\n",
       "                   %\"val_139\"<FLOAT,[None,None,None]> ⬅️ ::Transpose(%\"val_138\") {perm=[0, 2, 1]}\n",
       "            116 |  # node_Concat_194\n",
       "                   %\"val_140\"<INT64,[4]> ⬅️ ::Concat(%\"val_135\", %\"val_132\", %\"val_133\") {axis=0}\n",
       "            117 |  # node_Reshape_195\n",
       "                   %\"val_141\"<FLOAT,[None,None,None,None]> ⬅️ ::Reshape(%\"val_139\", %\"val_140\") {allowzero=0}\n",
       "            118 |  # node_Constant_301\n",
       "                   %\"val_142\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.4472136], dtype=float32), name='val_142')} ⬅️ ::Constant() {value=Tensor<FLOAT,[1]>(array([0.4472136], dtype=float32), name='val_142')}\n",
       "            119 |  # node_Mul_197\n",
       "                   %\"val_143\"<FLOAT,[1,8,s0,25]> ⬅️ ::Mul(%\"view_13\", %\"val_142\"{[0.4472135901451111]})\n",
       "            120 |  # node_Constant_304\n",
       "                   %\"val_145\"<FLOAT,[1]>{Tensor<FLOAT,[1]>(array([0.4472136], dtype=float32), name='val_145')} ⬅️ ::Constant() {value=Tensor<FLOAT,[1]>(array([0.4472136], dtype=float32), name='val_145')}\n",
       "            121 |  # node_Mul_200\n",
       "                   %\"val_146\"<FLOAT,[None,None,None,None]> ⬅️ ::Mul(%\"val_141\", %\"val_145\"{[0.4472135901451111]})\n",
       "            122 |  # node_MatMul_201\n",
       "                   %\"val_147\"<FLOAT,[None,8,s0,None]> ⬅️ ::MatMul(%\"val_143\", %\"val_146\")\n",
       "            123 |  # node_Softmax_202\n",
       "                   %\"val_148\"<FLOAT,[None,8,s0,None]> ⬅️ ::Softmax(%\"val_147\") {axis=-1}\n",
       "            124 |  # node_MatMul_204\n",
       "                   %\"scaled_dot_product_attention_1\"<FLOAT,[1,8,s0,25]> ⬅️ ::MatMul(%\"val_148\", %\"view_15\")\n",
       "            125 |  # node_Transpose_205\n",
       "                   %\"permute_1\"<FLOAT,[s0,1,8,25]> ⬅️ ::Transpose(%\"scaled_dot_product_attention_1\") {perm=[2, 0, 1, 3]}\n",
       "            126 |  # node_Concat_208\n",
       "                   %\"val_153\"<INT64,[2]> ⬅️ ::Concat(%\"val_0\", %\"val_11\"{[200]}) {axis=0}\n",
       "            127 |  # node_Reshape_210\n",
       "                   %\"view_16\"<FLOAT,[s0,200]> ⬅️ ::Reshape(%\"permute_1\", %\"val_153\") {allowzero=True}\n",
       "            128 |  # node_Gemm_211\n",
       "                   %\"linear_5\"<FLOAT,[s0,200]> ⬅️ ::Gemm(%\"view_16\", %\"encoder.layers.1.self_attn.out_proj.weight\"{...}, %\"encoder.layers.1.self_attn.out_proj.bias\"{...}) {transA=0, transB=True, alpha=1.0, beta=1.0}\n",
       "            129 |  # node_Concat_214\n",
       "                   %\"val_157\"<INT64,[3]> ⬅️ ::Concat(%\"val_0\", %\"val_9\"{[1]}, %\"val_11\"{[200]}) {axis=0}\n",
       "            130 |  # node_Reshape_216\n",
       "                   %\"view_17\"<FLOAT,[s0,1,200]> ⬅️ ::Reshape(%\"linear_5\", %\"val_157\") {allowzero=True}\n",
       "            131 |  # node_Transpose_217\n",
       "                   %\"transpose_11\"<FLOAT,[1,s0,200]> ⬅️ ::Transpose(%\"view_17\") {perm=[1, 0, 2]}\n",
       "            132 |  # node_Add_219\n",
       "                   %\"add_300\"<FLOAT,[1,s0,200]> ⬅️ ::Add(%\"layer_norm_1\", %\"transpose_11\")\n",
       "            133 |  # node_LayerNormalization_220\n",
       "                   %\"layer_norm_2\"<FLOAT,[1,s0,200]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_300\", %\"encoder.layers.1.norm1.weight\"{...}, %\"encoder.layers.1.norm1.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            134 |  # node_Transpose_221\n",
       "                   %\"val_161\"<FLOAT,[200,1024]> ⬅️ ::Transpose(%\"encoder.layers.1.linear1.weight\"{...}) {perm=[1, 0]}\n",
       "            135 |  # node_MatMul_222\n",
       "                   %\"val_162\"<FLOAT,[1,s0,1024]> ⬅️ ::MatMul(%\"layer_norm_2\", %\"val_161\")\n",
       "            136 |  # node_Add_223\n",
       "                   %\"linear_6\"<FLOAT,[1,s0,1024]> ⬅️ ::Add(%\"val_162\", %\"encoder.layers.1.linear1.bias\"{...})\n",
       "            137 |  # node_Relu_224\n",
       "                   %\"relu_1\"<FLOAT,[1,s0,1024]> ⬅️ ::Relu(%\"linear_6\")\n",
       "            138 |  # node_Transpose_226\n",
       "                   %\"val_163\"<FLOAT,[1024,200]> ⬅️ ::Transpose(%\"encoder.layers.1.linear2.weight\"{...}) {perm=[1, 0]}\n",
       "            139 |  # node_MatMul_227\n",
       "                   %\"val_164\"<FLOAT,[1,s0,200]> ⬅️ ::MatMul(%\"relu_1\", %\"val_163\")\n",
       "            140 |  # node_Add_228\n",
       "                   %\"linear_7\"<FLOAT,[1,s0,200]> ⬅️ ::Add(%\"val_164\", %\"encoder.layers.1.linear2.bias\"{...})\n",
       "            141 |  # node_Add_230\n",
       "                   %\"add_322\"<FLOAT,[1,s0,200]> ⬅️ ::Add(%\"layer_norm_2\", %\"linear_7\")\n",
       "            142 |  # node_LayerNormalization_231\n",
       "                   %\"layer_norm_3\"<FLOAT,[1,s0,200]>, %\"\"<?,?>, %\"\"<?,?> ⬅️ ::LayerNormalization(%\"add_322\", %\"encoder.layers.1.norm2.weight\"{...}, %\"encoder.layers.1.norm2.bias\"{...}) {axis=-1, epsilon=1e-05, stash_type=1}\n",
       "            143 |  # node_Constant_244\n",
       "                   %\"val_178\"<INT64,[]>{Tensor<INT64,[]>(array(-1, dtype=int64), name=None)} ⬅️ ::Constant() {value=Tensor<INT64,[]>(array(-1, dtype=int64), name=None)}\n",
       "            144 |  # node_Gather_245\n",
       "                   %\"select_6\"<FLOAT,[1,200]> ⬅️ ::Gather(%\"layer_norm_3\", %\"val_178\"{-1}) {axis=1}\n",
       "            145 |  # node_Gemm_257\n",
       "                   %\"linear_8\"<FLOAT,[1,100]> ⬅️ ::Gemm(%\"select_6\", %\"f.0.weight\"{...}, %\"f.0.bias\"{...}) {transA=0, transB=True, alpha=1.0, beta=1.0}\n",
       "            146 |  # node_Relu_258\n",
       "                   %\"relu_2\"<FLOAT,[1,100]> ⬅️ ::Relu(%\"linear_8\")\n",
       "            147 |  # node_Gemm_259\n",
       "                   %\"y\"<FLOAT,[1,7]> ⬅️ ::Gemm(%\"relu_2\", %\"f.2.weight\"{...}, %\"f.2.bias\"{[-0.039872098714113235, 0.11106158047914505, -0.10044346749782562, 0.003216438926756382, 0.005065219476819038, -0.16284292936325073, -0.14481626451015472]}) {transA=0, transB=True, alpha=1.0, beta=1.0}\n",
       "            return %\"y\"<FLOAT,[1,7]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_embedding_weight: \"f32[30187, 200]\", p_encoder_layer_self_attn_in_proj_weight: \"f32[600, 200]\", p_encoder_layer_self_attn_in_proj_bias: \"f32[600]\", p_encoder_layer_self_attn_out_proj_weight: \"f32[200, 200]\", p_encoder_layer_self_attn_out_proj_bias: \"f32[200]\", p_encoder_layer_linear1_weight: \"f32[1024, 200]\", p_encoder_layer_linear1_bias: \"f32[1024]\", p_encoder_layer_linear2_weight: \"f32[200, 1024]\", p_encoder_layer_linear2_bias: \"f32[200]\", p_encoder_layer_norm1_weight: \"f32[200]\", p_encoder_layer_norm1_bias: \"f32[200]\", p_encoder_layer_norm2_weight: \"f32[200]\", p_encoder_layer_norm2_bias: \"f32[200]\", p_encoder_layers_0_self_attn_in_proj_weight: \"f32[600, 200]\", p_encoder_layers_0_self_attn_in_proj_bias: \"f32[600]\", p_encoder_layers_0_self_attn_out_proj_weight: \"f32[200, 200]\", p_encoder_layers_0_self_attn_out_proj_bias: \"f32[200]\", p_encoder_layers_0_linear1_weight: \"f32[1024, 200]\", p_encoder_layers_0_linear1_bias: \"f32[1024]\", p_encoder_layers_0_linear2_weight: \"f32[200, 1024]\", p_encoder_layers_0_linear2_bias: \"f32[200]\", p_encoder_layers_0_norm1_weight: \"f32[200]\", p_encoder_layers_0_norm1_bias: \"f32[200]\", p_encoder_layers_0_norm2_weight: \"f32[200]\", p_encoder_layers_0_norm2_bias: \"f32[200]\", p_encoder_layers_1_self_attn_in_proj_weight: \"f32[600, 200]\", p_encoder_layers_1_self_attn_in_proj_bias: \"f32[600]\", p_encoder_layers_1_self_attn_out_proj_weight: \"f32[200, 200]\", p_encoder_layers_1_self_attn_out_proj_bias: \"f32[200]\", p_encoder_layers_1_linear1_weight: \"f32[1024, 200]\", p_encoder_layers_1_linear1_bias: \"f32[1024]\", p_encoder_layers_1_linear2_weight: \"f32[200, 1024]\", p_encoder_layers_1_linear2_bias: \"f32[200]\", p_encoder_layers_1_norm1_weight: \"f32[200]\", p_encoder_layers_1_norm1_bias: \"f32[200]\", p_encoder_layers_1_norm2_weight: \"f32[200]\", p_encoder_layers_1_norm2_bias: \"f32[200]\", p_f_0_weight: \"f32[100, 200]\", p_f_0_bias: \"f32[100]\", p_f_2_weight: \"f32[7, 100]\", p_f_2_bias: \"f32[7]\", x: \"i64[1, s0]\"):\n",
       "                     # \n",
       "                    sym_size_int_11: \"Sym(s0)\" = torch.ops.aten.sym_size.int(x, 1)\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190 in forward, code: return F.embedding(\n",
       "                    embedding: \"f32[1, s0, 200]\" = torch.ops.aten.embedding.default(p_embedding_weight, x, 0);  p_embedding_weight = x = None\n",
       "            \n",
       "                     # File: C:\\PyAI\\mood_project\\NN.py:37 in forward, code: x = self.embedding(x) * math.sqrt(self.d_model)\n",
       "                    mul_3: \"f32[1, s0, 200]\" = torch.ops.aten.mul.Tensor(embedding, 14.142135623730951);  embedding = None\n",
       "            \n",
       "                     # File: C:\\PyAI\\mood_project\\NN.py:15 in forward, code: div_term = torch.exp(torch.arange(0,self.d_model,2).float() * (-math.log(10000.0) / self.d_model)) #[0, 2, 4, 6, 8, ...]\n",
       "                    arange_1: \"i64[100]\" = torch.ops.aten.arange.start_step(0, 200, 2, device = device(type='cpu'), pin_memory = False);  arange_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_1: \"f32[1, s0, 200]\" = torch.ops.aten.clone.default(mul_3);  mul_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339 in forward, code: query = key = value = query.transpose(1, 0)\n",
       "                    transpose: \"f32[s0, 1, 200]\" = torch.ops.aten.transpose.int(clone_1, 1, 0)\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1373 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
       "                    linear: \"f32[s0, 1, 600]\" = torch.ops.aten.linear.default(transpose, p_encoder_layers_0_self_attn_in_proj_weight, p_encoder_layers_0_self_attn_in_proj_bias);  transpose = p_encoder_layers_0_self_attn_in_proj_weight = p_encoder_layers_0_self_attn_in_proj_bias = None\n",
       "                    view: \"f32[s0, 1, 3, 200]\" = torch.ops.aten.view.default(linear, [sym_size_int_11, 1, 3, 200]);  linear = None\n",
       "                    unsqueeze_3: \"f32[1, s0, 1, 3, 200]\" = torch.ops.aten.unsqueeze.default(view, 0);  view = None\n",
       "                    transpose_1: \"f32[3, s0, 1, 1, 200]\" = torch.ops.aten.transpose.int(unsqueeze_3, 0, -2);  unsqueeze_3 = None\n",
       "                    squeeze: \"f32[3, s0, 1, 200]\" = torch.ops.aten.squeeze.dim(transpose_1, -2);  transpose_1 = None\n",
       "                    clone_2: \"f32[3, s0, 1, 200]\" = torch.ops.aten.clone.default(squeeze, memory_format = torch.contiguous_format);  squeeze = None\n",
       "                    select: \"f32[s0, 1, 200]\" = torch.ops.aten.select.int(clone_2, 0, 0)\n",
       "                    select_1: \"f32[s0, 1, 200]\" = torch.ops.aten.select.int(clone_2, 0, 1)\n",
       "                    select_2: \"f32[s0, 1, 200]\" = torch.ops.aten.select.int(clone_2, 0, 2);  clone_2 = None\n",
       "                    view_1: \"f32[s0, 8, 25]\" = torch.ops.aten.view.default(select, [sym_size_int_11, 8, 25]);  select = None\n",
       "                    transpose_2: \"f32[8, s0, 25]\" = torch.ops.aten.transpose.int(view_1, 0, 1);  view_1 = None\n",
       "                    view_2: \"f32[s0, 8, 25]\" = torch.ops.aten.view.default(select_1, [sym_size_int_11, 8, 25]);  select_1 = None\n",
       "                    transpose_3: \"f32[8, s0, 25]\" = torch.ops.aten.transpose.int(view_2, 0, 1);  view_2 = None\n",
       "                    view_3: \"f32[s0, 8, 25]\" = torch.ops.aten.view.default(select_2, [sym_size_int_11, 8, 25]);  select_2 = None\n",
       "                    transpose_4: \"f32[8, s0, 25]\" = torch.ops.aten.transpose.int(view_3, 0, 1);  view_3 = None\n",
       "                    view_4: \"f32[1, 8, s0, 25]\" = torch.ops.aten.view.default(transpose_2, [1, 8, sym_size_int_11, 25]);  transpose_2 = None\n",
       "                    view_5: \"f32[1, 8, s0, 25]\" = torch.ops.aten.view.default(transpose_3, [1, 8, sym_size_int_11, 25]);  transpose_3 = None\n",
       "                    view_6: \"f32[1, 8, s0, 25]\" = torch.ops.aten.view.default(transpose_4, [1, 8, sym_size_int_11, 25]);  transpose_4 = None\n",
       "                    scaled_dot_product_attention: \"f32[1, 8, s0, 25]\" = torch.ops.aten.scaled_dot_product_attention.default(view_4, view_5, view_6);  view_4 = view_5 = view_6 = None\n",
       "                    permute: \"f32[s0, 1, 8, 25]\" = torch.ops.aten.permute.default(scaled_dot_product_attention, [2, 0, 1, 3]);  scaled_dot_product_attention = None\n",
       "                    view_7: \"f32[s0, 200]\" = torch.ops.aten.view.default(permute, [sym_size_int_11, 200]);  permute = None\n",
       "                    linear_1: \"f32[s0, 200]\" = torch.ops.aten.linear.default(view_7, p_encoder_layers_0_self_attn_out_proj_weight, p_encoder_layers_0_self_attn_out_proj_bias);  view_7 = p_encoder_layers_0_self_attn_out_proj_weight = p_encoder_layers_0_self_attn_out_proj_bias = None\n",
       "                    view_8: \"f32[s0, 1, 200]\" = torch.ops.aten.view.default(linear_1, [sym_size_int_11, 1, 200]);  linear_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
       "                    transpose_5: \"f32[1, s0, 200]\" = torch.ops.aten.transpose.int(view_8, 1, 0);  view_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_3: \"f32[1, s0, 200]\" = torch.ops.aten.clone.default(transpose_5);  transpose_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:913 in forward, code: x\n",
       "                    add_173: \"f32[1, s0, 200]\" = torch.ops.aten.add.Tensor(clone_1, clone_3);  clone_1 = clone_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm: \"f32[1, s0, 200]\" = torch.ops.aten.layer_norm.default(add_173, [200], p_encoder_layers_0_norm1_weight, p_encoder_layers_0_norm1_bias);  add_173 = p_encoder_layers_0_norm1_weight = p_encoder_layers_0_norm1_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_2: \"f32[1, s0, 1024]\" = torch.ops.aten.linear.default(layer_norm, p_encoder_layers_0_linear1_weight, p_encoder_layers_0_linear1_bias);  p_encoder_layers_0_linear1_weight = p_encoder_layers_0_linear1_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916 in forward, code: x = self.norm2(x + self._ff_block(x))\n",
       "                    relu: \"f32[1, s0, 1024]\" = torch.ops.aten.relu.default(linear_2);  linear_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_4: \"f32[1, s0, 1024]\" = torch.ops.aten.clone.default(relu);  relu = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_3: \"f32[1, s0, 200]\" = torch.ops.aten.linear.default(clone_4, p_encoder_layers_0_linear2_weight, p_encoder_layers_0_linear2_bias);  clone_4 = p_encoder_layers_0_linear2_weight = p_encoder_layers_0_linear2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_5: \"f32[1, s0, 200]\" = torch.ops.aten.clone.default(linear_3);  linear_3 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916 in forward, code: x = self.norm2(x + self._ff_block(x))\n",
       "                    add_195: \"f32[1, s0, 200]\" = torch.ops.aten.add.Tensor(layer_norm, clone_5);  layer_norm = clone_5 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_1: \"f32[1, s0, 200]\" = torch.ops.aten.layer_norm.default(add_195, [200], p_encoder_layers_0_norm2_weight, p_encoder_layers_0_norm2_bias);  add_195 = p_encoder_layers_0_norm2_weight = p_encoder_layers_0_norm2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1339 in forward, code: query = key = value = query.transpose(1, 0)\n",
       "                    transpose_6: \"f32[s0, 1, 200]\" = torch.ops.aten.transpose.int(layer_norm_1, 1, 0)\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1373 in forward, code: attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
       "                    linear_4: \"f32[s0, 1, 600]\" = torch.ops.aten.linear.default(transpose_6, p_encoder_layers_1_self_attn_in_proj_weight, p_encoder_layers_1_self_attn_in_proj_bias);  transpose_6 = p_encoder_layers_1_self_attn_in_proj_weight = p_encoder_layers_1_self_attn_in_proj_bias = None\n",
       "                    view_9: \"f32[s0, 1, 3, 200]\" = torch.ops.aten.view.default(linear_4, [sym_size_int_11, 1, 3, 200]);  linear_4 = None\n",
       "                    unsqueeze_4: \"f32[1, s0, 1, 3, 200]\" = torch.ops.aten.unsqueeze.default(view_9, 0);  view_9 = None\n",
       "                    transpose_7: \"f32[3, s0, 1, 1, 200]\" = torch.ops.aten.transpose.int(unsqueeze_4, 0, -2);  unsqueeze_4 = None\n",
       "                    squeeze_1: \"f32[3, s0, 1, 200]\" = torch.ops.aten.squeeze.dim(transpose_7, -2);  transpose_7 = None\n",
       "                    clone_6: \"f32[3, s0, 1, 200]\" = torch.ops.aten.clone.default(squeeze_1, memory_format = torch.contiguous_format);  squeeze_1 = None\n",
       "                    select_3: \"f32[s0, 1, 200]\" = torch.ops.aten.select.int(clone_6, 0, 0)\n",
       "                    select_4: \"f32[s0, 1, 200]\" = torch.ops.aten.select.int(clone_6, 0, 1)\n",
       "                    select_5: \"f32[s0, 1, 200]\" = torch.ops.aten.select.int(clone_6, 0, 2);  clone_6 = None\n",
       "                    view_10: \"f32[s0, 8, 25]\" = torch.ops.aten.view.default(select_3, [sym_size_int_11, 8, 25]);  select_3 = None\n",
       "                    transpose_8: \"f32[8, s0, 25]\" = torch.ops.aten.transpose.int(view_10, 0, 1);  view_10 = None\n",
       "                    view_11: \"f32[s0, 8, 25]\" = torch.ops.aten.view.default(select_4, [sym_size_int_11, 8, 25]);  select_4 = None\n",
       "                    transpose_9: \"f32[8, s0, 25]\" = torch.ops.aten.transpose.int(view_11, 0, 1);  view_11 = None\n",
       "                    view_12: \"f32[s0, 8, 25]\" = torch.ops.aten.view.default(select_5, [sym_size_int_11, 8, 25]);  select_5 = None\n",
       "                    transpose_10: \"f32[8, s0, 25]\" = torch.ops.aten.transpose.int(view_12, 0, 1);  view_12 = None\n",
       "                    view_13: \"f32[1, 8, s0, 25]\" = torch.ops.aten.view.default(transpose_8, [1, 8, sym_size_int_11, 25]);  transpose_8 = None\n",
       "                    view_14: \"f32[1, 8, s0, 25]\" = torch.ops.aten.view.default(transpose_9, [1, 8, sym_size_int_11, 25]);  transpose_9 = None\n",
       "                    view_15: \"f32[1, 8, s0, 25]\" = torch.ops.aten.view.default(transpose_10, [1, 8, sym_size_int_11, 25]);  transpose_10 = None\n",
       "                    scaled_dot_product_attention_1: \"f32[1, 8, s0, 25]\" = torch.ops.aten.scaled_dot_product_attention.default(view_13, view_14, view_15);  view_13 = view_14 = view_15 = None\n",
       "                    permute_1: \"f32[s0, 1, 8, 25]\" = torch.ops.aten.permute.default(scaled_dot_product_attention_1, [2, 0, 1, 3]);  scaled_dot_product_attention_1 = None\n",
       "                    view_16: \"f32[s0, 200]\" = torch.ops.aten.view.default(permute_1, [sym_size_int_11, 200]);  permute_1 = None\n",
       "                    linear_5: \"f32[s0, 200]\" = torch.ops.aten.linear.default(view_16, p_encoder_layers_1_self_attn_out_proj_weight, p_encoder_layers_1_self_attn_out_proj_bias);  view_16 = p_encoder_layers_1_self_attn_out_proj_weight = p_encoder_layers_1_self_attn_out_proj_bias = None\n",
       "                    view_17: \"f32[s0, 1, 200]\" = torch.ops.aten.view.default(linear_5, [sym_size_int_11, 1, 200]);  linear_5 = sym_size_int_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1395 in forward, code: return attn_output.transpose(1, 0), attn_output_weights\n",
       "                    transpose_11: \"f32[1, s0, 200]\" = torch.ops.aten.transpose.int(view_17, 1, 0);  view_17 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_7: \"f32[1, s0, 200]\" = torch.ops.aten.clone.default(transpose_11);  transpose_11 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:913 in forward, code: x\n",
       "                    add_300: \"f32[1, s0, 200]\" = torch.ops.aten.add.Tensor(layer_norm_1, clone_7);  layer_norm_1 = clone_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_2: \"f32[1, s0, 200]\" = torch.ops.aten.layer_norm.default(add_300, [200], p_encoder_layers_1_norm1_weight, p_encoder_layers_1_norm1_bias);  add_300 = p_encoder_layers_1_norm1_weight = p_encoder_layers_1_norm1_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_6: \"f32[1, s0, 1024]\" = torch.ops.aten.linear.default(layer_norm_2, p_encoder_layers_1_linear1_weight, p_encoder_layers_1_linear1_bias);  p_encoder_layers_1_linear1_weight = p_encoder_layers_1_linear1_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916 in forward, code: x = self.norm2(x + self._ff_block(x))\n",
       "                    relu_1: \"f32[1, s0, 1024]\" = torch.ops.aten.relu.default(linear_6);  linear_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_8: \"f32[1, s0, 1024]\" = torch.ops.aten.clone.default(relu_1);  relu_1 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_7: \"f32[1, s0, 200]\" = torch.ops.aten.linear.default(clone_8, p_encoder_layers_1_linear2_weight, p_encoder_layers_1_linear2_bias);  clone_8 = p_encoder_layers_1_linear2_weight = p_encoder_layers_1_linear2_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_9: \"f32[1, s0, 200]\" = torch.ops.aten.clone.default(linear_7);  linear_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:916 in forward, code: x = self.norm2(x + self._ff_block(x))\n",
       "                    add_322: \"f32[1, s0, 200]\" = torch.ops.aten.add.Tensor(layer_norm_2, clone_9);  layer_norm_2 = clone_9 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217 in forward, code: return F.layer_norm(\n",
       "                    layer_norm_3: \"f32[1, s0, 200]\" = torch.ops.aten.layer_norm.default(add_322, [200], p_encoder_layers_1_norm2_weight, p_encoder_layers_1_norm2_bias);  add_322 = p_encoder_layers_1_norm2_weight = p_encoder_layers_1_norm2_bias = None\n",
       "            \n",
       "                     # File: C:\\PyAI\\mood_project\\NN.py:40 in forward, code: x = x[:,-1,:] #transformer의 계산 결과과 형태는 rnn과 같으나, rnn하고 다르게, transformer 는 병렬계산이기 때문에 아무 값 가져와도 됩니다.\n",
       "                    slice_15: \"f32[1, s0, 200]\" = torch.ops.aten.slice.Tensor(layer_norm_3, 0, 0, 9223372036854775807);  layer_norm_3 = None\n",
       "                    select_6: \"f32[1, 200]\" = torch.ops.aten.select.int(slice_15, 1, -1);  slice_15 = None\n",
       "                    slice_16: \"f32[1, 200]\" = torch.ops.aten.slice.Tensor(select_6, 1, 0, 9223372036854775807);  select_6 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_8: \"f32[1, 100]\" = torch.ops.aten.linear.default(slice_16, p_f_0_weight, p_f_0_bias);  slice_16 = p_f_0_weight = p_f_0_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:133 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_2: \"f32[1, 100]\" = torch.ops.aten.relu.default(linear_8);  linear_8 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Administrator\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear_9: \"f32[1, 7]\" = torch.ops.aten.linear.default(relu_2, p_f_2_weight, p_f_2_bias);  relu_2 = p_f_2_weight = p_f_2_bias = None\n",
       "                    return (linear_9,)\n",
       "            \n",
       "        Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_embedding_weight'), target='embedding.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_self_attn_in_proj_weight'), target='encoder_layer.self_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_self_attn_in_proj_bias'), target='encoder_layer.self_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_self_attn_out_proj_weight'), target='encoder_layer.self_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_self_attn_out_proj_bias'), target='encoder_layer.self_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_linear1_weight'), target='encoder_layer.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_linear1_bias'), target='encoder_layer.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_linear2_weight'), target='encoder_layer.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_linear2_bias'), target='encoder_layer.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_norm1_weight'), target='encoder_layer.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_norm1_bias'), target='encoder_layer.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_norm2_weight'), target='encoder_layer.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layer_norm2_bias'), target='encoder_layer.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_self_attn_in_proj_weight'), target='encoder.layers.0.self_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_self_attn_in_proj_bias'), target='encoder.layers.0.self_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_self_attn_out_proj_weight'), target='encoder.layers.0.self_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_self_attn_out_proj_bias'), target='encoder.layers.0.self_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_linear1_weight'), target='encoder.layers.0.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_linear1_bias'), target='encoder.layers.0.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_linear2_weight'), target='encoder.layers.0.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_linear2_bias'), target='encoder.layers.0.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_norm1_weight'), target='encoder.layers.0.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_norm1_bias'), target='encoder.layers.0.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_norm2_weight'), target='encoder.layers.0.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_0_norm2_bias'), target='encoder.layers.0.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_self_attn_in_proj_weight'), target='encoder.layers.1.self_attn.in_proj_weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_self_attn_in_proj_bias'), target='encoder.layers.1.self_attn.in_proj_bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_self_attn_out_proj_weight'), target='encoder.layers.1.self_attn.out_proj.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_self_attn_out_proj_bias'), target='encoder.layers.1.self_attn.out_proj.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_linear1_weight'), target='encoder.layers.1.linear1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_linear1_bias'), target='encoder.layers.1.linear1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_linear2_weight'), target='encoder.layers.1.linear2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_linear2_bias'), target='encoder.layers.1.linear2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_norm1_weight'), target='encoder.layers.1.norm1.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_norm1_bias'), target='encoder.layers.1.norm1.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_norm2_weight'), target='encoder.layers.1.norm2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_encoder_layers_1_norm2_bias'), target='encoder.layers.1.norm2.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_f_0_weight'), target='f.0.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_f_0_bias'), target='f.0.bias', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_f_2_weight'), target='f.2.weight', persistent=None), InputSpec(kind=<InputKind.PARAMETER: 2>, arg=TensorArgument(name='p_f_2_bias'), target='f.2.bias', persistent=None), InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='linear_9'), target=None)])\n",
       "        Range constraints: {s0: VR[2, int_oo]}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# onnx로 변환하기\n",
    "x = torch.randint(0, 30187, (1,30)).type(torch.long) # 더미 입력값, 문장하나 & 단어 25개\n",
    "dynamic_axes = {'x' : {0 : 'b', 1 : 'f'}, 'y' : {0 : 'b'}}\n",
    "\n",
    "encoder.eval() #혹시 모르니 명시적으로 dropout은 꺼주자\n",
    "torch.onnx.export(\n",
    "    encoder, # 바꿀 torch 신경망\n",
    "    x, # 더미 입력값\n",
    "    \"encoder.onnx\", # onnx 저장 파일 경로 \n",
    "    export_params=True, # weight, bias 값들 저장할지\n",
    "    opset_version=20,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['x'],\n",
    "    output_names=['y'],\n",
    "    dynamic_axes=dynamic_axes, # 가변길이 차원\n",
    "    dynamo=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d638f38-4291-4ffd-9a6b-711d06f6677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_encoder = onnx.load('encoder.onnx')\n",
    "\n",
    "onnx.checker.check_model(onnx_encoder) # 문제가 없으면 아무 메세지 출력하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1bb769b-2b4a-4064-ad51-c1e7c5bd3254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n",
      "(1, 7)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "onnx_F = onnxruntime.InferenceSession(\"encoder.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "np_x = np.random.randint(0, 30187, (1,22)).astype(np.int64) #문장 2개에 각 문장당 단어 10개\n",
    "tensor_x = torch.tensor(np_x, dtype = torch.long)\n",
    "\n",
    "tensor_y = encoder(tensor_x)\n",
    "print(tensor_y.shape)\n",
    "\n",
    "inputs = {onnx_F.get_inputs()[0].name : np_x}\n",
    "outs = onnx_F.run(None, inputs)\n",
    "np_y = outs[0]\n",
    "print(np_y.shape)\n",
    "\n",
    "\n",
    "np.testing.assert_allclose(tensor_y.detach().numpy(), np_y, rtol=1e-03, atol=1e-05) #오류 메세지가 없으면 오차범위 내에서만 차이가 있는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1c6ac-a1d0-4464-a545-7f04fefd1d07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
